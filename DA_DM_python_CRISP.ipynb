{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2019-3-19 12:33:47\n",
    "> integrate data_analysis_python.ipynb with CRISP-DM framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outline\n",
    "\n",
    "- 1. 从mysql里提数据时，注意要判断主特征与另外哪些特征相关。\n",
    "    - 比如近期从提取的数据没有考虑到进餐标记。\n",
    "    - 最快的考察时groupby 此特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- Scipy: https://docs.scipy.org/doc/scipy/reference/genindex.html\n",
    "- Numpy: https://docs.scipy.org/doc/numpy/genindex.html\n",
    "- Pandas: http://pandas.pydata.org/pandas-docs/stable/genindex.html\n",
    "- Matplotlib: https://matplotlib.org/genindex.html\n",
    "- tslern: https://tslearn.readthedocs.io/en/latest/auto_examples/index.html\n",
    "- StatsModel: http://www.statsmodels.org/stable/index.html\n",
    "- pyflux: https://pyflux.readthedocs.io/en/latest/getting_started.html#step-four-evaluate-model-fit\n",
    "\n",
    "- CA683_DataAnalytics_DataMining\n",
    "- CA659_MathmeticalMethod\n",
    "- https://otexts.com/fpp2/\n",
    "    - CRISP detail: https://otexts.com/fpp2/basic-steps.html\n",
    "- Textbook: Python for Data Analysis\n",
    "- https://anomaly.io/moving-median-robust-anomaly/\n",
    "- YouDaoNote: \n",
    "- https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/\n",
    "- https://pbpython.com/\n",
    "- https://pyflux.readthedocs.io/en/latest/index.html\n",
    "- https://ml-cheatsheet.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem recognition/Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Business Objectives\n",
    "- About the classification problem:\n",
    "    - which class is most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Data Mining Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Raw Data\n",
    "\n",
    "- long format for multiple time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data from dir\n",
    "```\n",
    "def load_dir(dir_path):\n",
    "  \"\"\"\n",
    "  read a dir into one dataframe\n",
    "  \"\"\"\n",
    "  \n",
    "  \n",
    "  dir_list = os.listdir(dir_path)\n",
    "  \n",
    "  pbar = pyprind.ProgBar(int(len(dir_list)), title='load_dir')\n",
    "  \n",
    "  feature_list = []\n",
    "  for filename in dir_list:\n",
    "    \n",
    "    file_route = os.path.join(dir_path,filename) \n",
    "    row_name = os.path.splitext(filename)[0]\n",
    "    df = pd.read_csv(file_route,header=None,sep=' ')\n",
    "    df.rename({0:row_name},axis = 'index',inplace=True)\n",
    "    feature_list.append(df)\n",
    "    \n",
    "    pbar.update()                     \n",
    "  \n",
    "  return pd.concat(feature_list,axis='index')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_csv\n",
    "\n",
    "- windows: CRLF, Linux: LF\n",
    "- LF: '\\n'\n",
    "- CR: '\\r'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read_xml\n",
    "\n",
    "- https://medium.com/@robertopreste/from-xml-to-pandas-dataframes-9292980b1c1c\n",
    "- https://stackoverflow.com/questions/47917787/xml-etree-elementtree-parseerror-exception-handling-not-catching-errors\n",
    "\n",
    "```\n",
    "EX1: Dataset_Explore: weibo\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tslearn.datasets\n",
    "\n",
    "```\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "\n",
    "datasets = datasets = UCR_UEA_datasets()\n",
    "\n",
    "dataset_list = datasets.list_datasets()\n",
    "\n",
    "output = 'ucr_uea_datasets_list.txt'\n",
    "with open(join(PROJECT_PATH,output),'w+') as f:\n",
    "  f.write(str(dataset_list))\n",
    "\n",
    "> see content/drive/My Drive/DCU/practicum/ucr_uea_datasets_list.txt\n",
    "\n",
    "ecg200 = datasets.load_dataset('ECG200')\n",
    "\n",
    "# Directly unpack a tuple, return 4 numpy.ndarray\n",
    "X_train, y_train, X_test, y_test = ecg200\n",
    "\n",
    "X_train.shape\n",
    "> (100, 96, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data from mysql\n",
    "\n",
    "```\n",
    "# install sqlalchemy, pymysql\n",
    "\n",
    "import MySQLdb\n",
    "import pandas.io.sql as psql\n",
    "from sqlalchemy import create_engine \n",
    "from mysignin import signin\n",
    "\n",
    "\n",
    "# Step 1: connect to database\n",
    "    \"\"\"mysignin.py\n",
    "    import pandas as pd\n",
    "\n",
    "    # https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "    # https://stackoverflow.com/questions/42163470/how-to-execute-a-py-file-from-a-ipynb-file-on-the-jupyter-notebook\n",
    "    def signin(db_name):\n",
    "        \"\"\"\n",
    "        :db_name: database name you would like to explore\n",
    "        :return: the serverURL for cearte engine\n",
    "        \"\"\"\n",
    "        log_info = pd.read_csv('E:/up.csv',index_col='server',dtype='str')\n",
    "        server = 'seer_mysql'\n",
    "        user = log_info.loc[server,'user']\n",
    "        pw = log_info.loc[server,'password']\n",
    "        host = log_info.loc[server,'host']\n",
    "        port = log_info.loc[server,'port']\n",
    "        serverURL = \"\".join((\"mysql+pymysql://\",user,\":\",pw,\"@\",host,\":\",port,\"/\",db_name))\n",
    "\n",
    "        return serverURL\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Step 3, read data by sql query to dataframe\n",
    "def etl(server_name,db_name, query):\n",
    "    URL = signin(server_name =server_name, db_name=db_name)\n",
    "    eng = create_engine(URL)\n",
    "    df = pd.read_sql_query(query,eng)\n",
    "    \n",
    "    # close engine after each query\n",
    "    eng.dispose() # https://stackoverflow.com/questions/42034373/does-pandas-need-to-close-connection\n",
    "    return df.head()\n",
    "\n",
    "\n",
    "def main():\n",
    "    server_name = \"seer_report\"\n",
    "    db_name = \"report\"\n",
    "    qry = \"SELECT * FROM report.report_disease LIMIT 20;\"\n",
    "    ans = etl(server_name=server_name,db_name=db_name,query=qry)\n",
    "    print(ans)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# utils\n",
    "def run_query(query):\n",
    "    with engine.begin() as conn:\n",
    "        for row in conn.execute(query):\n",
    "            print(row)\n",
    "\n",
    "```\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mysql select order:\n",
    "```\n",
    "SELECT\n",
    "SELECT col1, col2\n",
    "FROM table\n",
    "WHERE condition\n",
    "GROUP BY cols\n",
    "HAVING condition\n",
    "ORDER BY col;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Data\n",
    "- index or columns are all the pandas datatype.\n",
    "    - df.index\n",
    "    - df.columns\n",
    "    - df.dtypes\n",
    "- append Concatenate with additional Index objects, producing a new Index\n",
    "- difference Compute set difference as an Index\n",
    "- intersection Compute set intersection\n",
    "- union Compute set union\n",
    "- isin Compute boolean array indicating whether each value is contained in the passed collection\n",
    "- delete Compute new Index with element at index i deleted\n",
    "- drop Compute new Index by deleting passed values\n",
    "- insert Compute new Index by inserting element at index i\n",
    "- is_monotonic Returns True if each element is greater than or equal to the previous element\n",
    "- is_unique Returns True if the Index has no duplicate values\n",
    "- unique Compute the array of unique values in the Index\n",
    "- value_counts()\n",
    "- isin Compute boolean array indicating whether each Series value is contained in the passed sequence ofvalues\n",
    "- match Compute integer indices for each value in an array into another array of distinct values; helpful for data alignment and join-type operations\n",
    "- unique Compute array of unique values in a Series, returned in the order observed\n",
    "- value_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in descending order\n",
    "- series.between\n",
    "\n",
    "> py for da, page 160, table 5-8\n",
    "- count Number of non-NA values\n",
    "- describe Compute set of summary statistics for Series or each DataFrame column\n",
    "- min, max Compute minimum and maximum values\n",
    "- argmin, argmax Compute index locations (integers) at which minimum or maximum value obtained, respectively\n",
    "- idxmin, idxmax Compute index labels at which minimum or maximum value obtained, respectively\n",
    "- quantile Compute sample quantile ranging from 0 to 1\n",
    "- sum Sum of values\n",
    "- mean Mean of values\n",
    "- median Arithmetic median (50% quantile) of values\n",
    "- mad Mean absolute deviation from mean value\n",
    "- prod Product of all values\n",
    "\n",
    "- var Sample variance of values\n",
    "\\begin{equation*}\n",
    "\\sigma^2 (x) = E[(X-E(X))^2] =  E[(x-\\mu)^2]\n",
    "\\end{equation*}\n",
    "\n",
    "- std Sample standard deviation of values\n",
    "\\begin{equation*}\n",
    "\\sigma(x) = \\sqrt{E[(X-E(X))^2] =  E[(x-\\mu)^2]}\n",
    "\\end{equation*}\n",
    "\n",
    "- skew Sample skewness (third moment) of values\n",
    "    - skewness is a measure of the asymmetry of the probability distribution\n",
    "    - negative skew commonly indicates that the tail is on the left side of the distribution,\n",
    "    - positive skew indicates that the tail is on the right\n",
    "\\begin{equation*}\n",
    "skew(X) = E[(\\frac{x-\\mu}{\\sigma})^3]\n",
    "\\end{equation*}    \n",
    "\n",
    "- kurt Sample kurtosis (fourth moment) of values\n",
    "    -  person:  the fourth moment \n",
    "        - related to the tails of the distribution\n",
    "        - the sometimes-seen characterization as \"peakedness\" is mistaken\n",
    "\\begin{equation*}\n",
    "kurt(X) =  E[(\\frac{x-\\mu}{\\sigma})^4]\n",
    "\\end{equation*}\n",
    "- cumsum Cumulative sum of values\n",
    "- cummin, cummax Cumulative minimum or maximum of values, respectively\n",
    "- cumprod Cumulative product of values\n",
    "- diff Compute first arithmetic difference (useful for time series)\n",
    "    - https://machinelearningmastery.com/difference-time-series-dataset-python/\n",
    "- pct_change Compute percent changes\n",
    "- cov Covariance(协方差): how one variable varies with a second\n",
    "    - df.cov Covariance Matrix\n",
    "        - Compute pairwise covariance of columns, excluding NA/null values.\n",
    "        - return covariance matrix \n",
    "    - if cov is 0: then X and Y is indepedent\n",
    "\\begin{equation*}\n",
    "COV(x,y) = E[(x-\\mu_x)(y - \\mu_y)]\n",
    "\\end{equation*}\n",
    "\n",
    "- Sample Covariance:\n",
    "\\begin{equation*}\n",
    "COV(x,y) = \\frac{\\sum(x_i - \\mu_x)(y_i - \\mu_y)}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "- Correlation(相关系数): scales the covariance\n",
    "    - Correlation is a kind of normalized covariance, with a value between -1 and 1\n",
    "\\begin{equation*}\n",
    "\\rho(x,y) = COV(x,y) / \\sigma(x)\\sigma(y)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Method UML\n",
    "- UML\n",
    "    - https://openclassrooms.com/en/courses/4191736-design-a-database-with-uml/4191743-learn-about-class-diagrams\n",
    "    - draw.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Components of a Time Series\n",
    "- Trend:  The general direction in which the series is running during a long period\n",
    "- Seasonality: A seasonal pattern occurs when a time series is accted by seasonal factors such as the time of the year or the day of the week.\n",
    "- Cyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency.\n",
    "    - The duration of these fluctuations is usually at least 2 years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Autocorrelation\n",
    "- signal correlation with itself at various t\n",
    "    - [ ]the relationship with DFT\n",
    "- There are several autocorrelation coefficients,\n",
    "- pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity\n",
    "- A time series is first order stationary iff It is stationary in the mean (i.e. has constant mean).\n",
    "- A time series is second order stationary iff It is stationary in the mean & variance (constant mean, variance, covariance).\n",
    "- time series with trends/ seasonality, are not stationary\n",
    "- white noise series is stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib\n",
    "- multiple plot https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html\n",
    "- https://matplotlib.org/api/api_overview.html\n",
    "- colors: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "- s1. Get fig and axes object and initial them\n",
    "\n",
    "```\n",
    "# use Grid Axes to arrange the figure.\n",
    "widths = [4]\n",
    "heights = [1,1,1,1,1] \n",
    "\n",
    "gs_kw = dict(width_ratios=widths,height_ratios=heights)\n",
    "fig,axes = plt.subplots(nrows=5,ncols=1,figsize=[18,20],\n",
    "                        constrained_layout=True,gridspec_kw = gs_kw)\n",
    "\n",
    "main_title = \"Capital String\"\n",
    "fig.suptitle(main_title,y=1.0,fontsize=50)\n",
    "\n",
    "```\n",
    "\n",
    "- s2. def the plot function\n",
    "    - https://matplotlib.org/api/axes_api.html#the-axes-class \n",
    "\n",
    "```\n",
    "def data_plot(ax,data):\n",
    "    - ax.plot\n",
    "    \n",
    "```\n",
    "- s3. show plot:\n",
    "\n",
    "```\n",
    "plt.tight_layout()\n",
    "plt.show\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map(pyecharts)\n",
    "- https://pyecharts.org/#/zh-cn/intro\n",
    "\n",
    "```\n",
    "from pyecharts.charts import Map\n",
    "from pyecharts import options as opts\n",
    "\n",
    "map_data = user_df['location'].value_counts()\n",
    "data = user_df['location'].value_counts().loc[lambda x: x>10]\n",
    "location = list(data.index)\n",
    "values = [int(x) for x in data.values]\n",
    "data_pair= [list(z) for z in zip(location,values)],\n",
    "\n",
    "map1 = Map(init_opts=opts.InitOpts(width=\"1200px\", height=\"600px\"))\n",
    "map1.add(\"\",\n",
    "         data_pair= [z for z in zip(location,values)],\n",
    "         maptype = \"china-cities\",\n",
    "         label_opts=opts.LabelOpts(is_show=False),\n",
    "         is_map_symbol_show = True,\n",
    "         )\n",
    "map1.set_global_opts(visualmap_opts=opts.VisualMapOpts(),)\n",
    "map1.render_notebook() \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas.plot()\n",
    "\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\n",
    "- for quick plot pd.Series or pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot horizontal line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw a line by point-slope form\n",
    "\n",
    "```\n",
    "- mean = X.mean(axis=0)\n",
    "xp,yp = mean[0],mean[1]\n",
    "slope = 0.81410372/0.5807195 \n",
    "intercept = yp - slope*xp\n",
    "x = X[:,0]\n",
    "y = slope*x + intercept\n",
    "plt.plot(x,y)\n",
    "\n",
    "xp,yp = mean[0],mean[1]\n",
    "slope = -0.5807195/0.81410372\n",
    "intercept = yp - slope*xp\n",
    "x = X[:,0]\n",
    "y = slope*x + intercept\n",
    "plt.plot(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore \n",
    "\n",
    "> can use QA format\n",
    "\n",
    "- pct_change()\n",
    "- corr()\n",
    "- cov()\n",
    "- corrwith()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select one index values from multiIndex\n",
    "\n",
    "```\n",
    "df.index.get_level_values(index_level)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select rows by features' value_counts()\n",
    "\n",
    "``` \n",
    "# Example: 选择user_id_counts == 3 的用户\n",
    "user_id_counts = blood_datum_df['user_id'].value_counts() \n",
    "user_id_counts_3 = user_id_counts[user_id_counts == 3] # 选择出统计为3的用户id\n",
    "blood_user_3 = blood_datum_df.loc[blood_datum_df['user_id'].isin(list(user_id_counts_3.index)),:] #使用isin list 选择条目\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Identify the Distribution of Your Data\n",
    "- https://statisticsbyjim.com/hypothesis-testing/identify-distribution-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the most common value in ndarray\n",
    "- https://stackoverflow.com/questions/12297016/how-to-find-most-frequent-values-in-numpy-ndarray\n",
    "- indices: the location of num in array in the unique list(sorted)\n",
    "\n",
    "```\n",
    "arr = np.array([5, 4, -2, 1, -2, 0, 4, 4, -6, -1])\n",
    "u, indices = np.unique(arr, return_inverse=True)\n",
    "print(u,indices)\n",
    "print(np.bincount(indices))\n",
    "print(np.argmax(np.bincount(indices)))\n",
    "u[np.argmax(np.bincount(indices))]\n",
    "\n",
    "output:\n",
    "[-6 -2 -1  0  1  4  5] [6 5 1 4 1 3 5 5 0 2]\n",
    "[1 2 1 1 1 3 1]\n",
    "5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "- http://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n",
    "\n",
    "- By default the group keys are sorted during the groupby operation\n",
    "\n",
    "- in case of multiindex, you can use as_index = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting\n",
    "- groupby().get_groups('groupkey') : return group as df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching\n",
    "- groupby.groups\n",
    "    - return the entry index, not the groubyed column values, id rather than values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time-series \n",
    "\n",
    "#### Moving Average Smoothing For Trend\n",
    "- Weights for moving averages must always add to 1.\n",
    "\n",
    "- Simple Moving Average:\n",
    "\\begin{equation*}\n",
    "Y_{T+1,T} = \\alpha Y_t + \\alpha (\\alpha - 1) Y_{t-1} + \\alpha (\\alpha -1)^2 Y_{t-2} \n",
    "\\end{equation*}\n",
    "\n",
    "- 2* 4  = 5 pt Moving Average\n",
    "    - fisst 4point simple average\n",
    "    - then 2point simple average\n",
    "\n",
    "#### Exponential Smoothing\n",
    "\\begin{equation*}\n",
    "\\alpha,(1-\\alpha),(1-\\alpha)^2,(1-\\alpha)^3\n",
    "\\end{equation*}\n",
    "\n",
    "#### Seasonal plots:\n",
    "- data against seasonality,\n",
    "- many lines in one graph\n",
    "- variation: polar coordinates\n",
    "- variation: \n",
    "\n",
    "#### Scatter plot\n",
    "\n",
    "- x,y axis represent the two kinds features and scatter shape represent the category.\n",
    "\n",
    "#### Scatter plot matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag plots \n",
    "- panel 1 is the realationship between y_t and y_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACF and PACF plot \n",
    "- correlogram\n",
    "- trend:\n",
    "    - the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size.\n",
    "- seasonal: \n",
    "    - When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.\n",
    "- PACF cutoff after p lag , so it is AR(p) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data\n",
    "- http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\n",
    "- loc: Selection By Label\n",
    "    - A single label, e.g. 5 or 'a', \n",
    "    - A list or array of labels, e.g. ['a', 'b', 'c'].\n",
    "    - A slice object with labels, e.g. 'a':'f'.\n",
    "        - both the start and the stop are included\n",
    "    - A boolean array of the same length as the axis being sliced, e.g. [True, False, True].\n",
    "    - A callable function with one argument (the calling Series, DataFrame or Panel) and that returns valid output for indexing (one of the above)\n",
    " \n",
    "- df[val] Select single column or sequence of columns from the DataFrame; special case conveniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set values based on some criterion)\n",
    "- df.loc[val] Selects single row or subset of rows from the DataFrame by label\n",
    "- df.loc[:, val] Selects single column or subset of columns by label\n",
    "- df.loc[val1, val2] Select both rows and columns by label\n",
    "- df.iloc[where] Selects single row or subset of rows from the DataFrame by integer position\n",
    "- df.iloc[:, where] Selects single column or subset of columns by integer position\n",
    "- df.iloc[start:stop:step,:] Select all column and certain rows.\n",
    "- df.iloc[where_i, where_j] Select both rows and columns by integer position\n",
    "- df.at[label_i, label_j] Select a single scalar value by row and column label\n",
    "- df.iat[i, j] Select a single scalar value by row and column position (integers)\n",
    "- reindex method Select either rows or columns by labels\n",
    "- get_value, set_value methods Select single value by row and column label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "- Remaining data is seriously biased\n",
    "\n",
    "- Missing compeletly at random (MCAR):  the probability of being missing is the same for all cases\n",
    "    - Missing value (y) neither depends on x nor y\n",
    "- Missing at random (MAR): the probability of being missing is the same only within groups defined by the observed data\n",
    "    - Missing value (y) depends on x, but not y\n",
    "- Not Missing at random (NMAR): the probability of being missing varies for reasons that are unknown to us\n",
    "    - The probability of a missing value depends on the variable that is missing\n",
    "- https://stefvanbuuren.name/fimd/sec-MCAR.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "- [ ] 表的形态是否需要reshape or pivot\n",
    "    - file:///E:/Reference/cheatsheet/pandas/user_guide/reshaping.html\n",
    "- [ ] missing value\n",
    "    - 检查na, zero 以及二者的混合，通过检查column的mean值最方便，如果mean ==0;\n",
    "    - isna().all().values\n",
    "    - notna().all().values\n",
    "- [ ] 异常值 outliers\n",
    "- [ ] 数据类型\n",
    "    - object type or str type need to check wheather there are whitespaces at both ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete one row\n",
    "\n",
    "```\n",
    "df.drop(label=rawname)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string and text\n",
    "\n",
    "- python built-in\n",
    "    - count Return the number of non-overlapping occurrences of substring in the string.\n",
    "    - endswith Returns True if string ends with suffix.\n",
    "    - startswith Returns True if string starts with prefix.\n",
    "    - join Use string as delimiter for concatenating a sequence of other strings.\n",
    "    - index Return position of first character in substring if found in the string; raises ValueError if not found.\n",
    "    - find Return position of first character of first occurrence of substring in the string; like index, but returns –1\n",
    "    - if not found.\n",
    "    - rfind Return position of first character of last occurrence of substring in the string; returns –1 if not found.\n",
    "    - replace Replace occurrences of string with another string.\n",
    "    - strip,\n",
    "    - rstrip,\n",
    "    - lstrip\n",
    "    - Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively)\n",
    "    - for each element.\n",
    "    - split Break string into list of substrings using passed delimiter.\n",
    "    - lower Convert alphabet characters to lowercase.\n",
    "    - upper Convert alphabet characters to uppercase.\n",
    "    - casefold Convert characters to lowercase, and convert any region-specific variable character combinations to a\n",
    "     common comparable form.\n",
    "    - ljust,\n",
    "    - rjust\n",
    "    - Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill\n",
    "    - character) to return a string with a minimum width.\n",
    "\n",
    "- re module:\n",
    "    - findall Return all non-overlapping matching patterns in a string as a list\n",
    "    - finditer Like findall, but returns an iterator\n",
    "    - match Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, returns a match object, and otherwise None\n",
    "    - search Scan string for match to pattern; returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning\n",
    "    - split Break string into pieces at each occurrence of pattern\n",
    "    - sub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols \\1, \\2, ... to refer to match group elements in the replacement string\n",
    "- textBlob: spelling check\n",
    "    - https://textblob.readthedocs.io/en/dev/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value\n",
    "- dropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n",
    "- fillna Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\n",
    "- isnull Return boolean values indicating which values are missing/NA.\n",
    "- notnull Negation of isnull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(5,5),columns=['a', 'b', 'c', 'd', 'e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df > 0.9] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a      b      c      d      e\n",
       "0  False  False  False  False   True\n",
       "1  False  False  False  False  False\n",
       "2   True  False  False  False  False\n",
       "3  False  False   True  False  False\n",
       "4  False   True  False  False  False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    1\n",
       "c    1\n",
       "d    0\n",
       "e    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "- [ ] https://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_housing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Data\n",
    "- pd.reindex()\n",
    "    - index New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An\n",
    "    - Index will be used exactly as is without any copying.\n",
    "    - method Interpolation (fill) method; 'ffill' fills forward, while 'bfill' fills backward.\n",
    "    - fill_value Substitute value to use when introducing missing data by reindexing.\n",
    "    - limit When forward- or backfilling, maximum size gap (in number of elements) to fill.\n",
    "    - tolerance When forward- or backfilling, maximum size gap (in absolute numeric distance) to fill for inexact matches.\n",
    "    - level Match simple Index on level of MultiIndex; otherwise select subset of.\n",
    "    - copy If True, always copy underlying data even if new index is equivalent to old index; if False, do not copy the data when the indexes are equivalent.\n",
    "\n",
    "- pd.drop()\n",
    "    - drop values from the columns by passing axis=1 or axis='columns':\n",
    "- arithmetic between objects with different indexes.\n",
    "    - By default, arithmetic between DataFrame and Series matches the index of the Series on the DataFrame’s columns, broadcasting down the rows\n",
    "    - the union of the index pairs.\n",
    "    - similar to an automatic outer join on the index labels\n",
    "    - add: NaN if not in both df.\n",
    "    - add, radd Methods for addition (+)\n",
    "    - sub, rsub Methods for subtraction (-)\n",
    "    - div, rdiv Methods for division (/)\n",
    "    - floordiv, rfloordiv Methods for floor division (//)\n",
    "    - mul, rmul Methods for multiplication (*)\n",
    "    - pow, rpow Methods for exponentiation (**)\n",
    "- NumPy ufuncs\n",
    "    - npl.abs(df)\n",
    "    - https://docs.scipy.org/doc/numpy/reference/ufuncs.html\n",
    "- df.apply(f, axis='columns')\n",
    "    - take row as series\n",
    "- sort_index()\n",
    "- sort_values()\n",
    "- rank()\n",
    "    - 'average' Default: assign the average rank to each entry in the equal group\n",
    "    - 'min' Use the minimum rank for the whole group\n",
    "    - 'max' Use the maximum rank for the whole group\n",
    "    - 'first' Assign ranks in the order the values appear in the data\n",
    "    - 'dense' Like method='min', but ranks always increase by 1 in between groups rather than the number of equal\n",
    "    - elements in a group\n",
    "- rename()\n",
    "    - rename the index or colunm label\n",
    "- rename_axis()\n",
    "    - rename axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split one colunm to two columns\n",
    "\n",
    "```\n",
    "https://www.geeksforgeeks.org/split-a-text-column-into-two-columns-in-pandas-dataframe/\n",
    "\n",
    "blood_type_dfn[['high_pre','low_pre']] = blood_type_dfn.loc[:,'14'].str.split('/',expand=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "- Focuses on scalability, cost efficiency and fault-tolerance\n",
    "- Special-purpose computa7ons that process large amounts of raw data (by Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "- visualize high dimension data\n",
    "- reduce noise\n",
    "- preprocess before other algorithm\n",
    "- the eigenvector with the highest eigenvalue is the principle component of the data set!\n",
    "- eigenvector is direction of principle component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Discretization\n",
    "- converting continuous values of an attribute into categorical data or partitions or intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4-5 Rule\n",
    "- https://idevji.com/intuitive-partitioning-data-3-4-5-rule/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.merge(left,right,how=,on=)\n",
    "- - https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "- left DataFrame to be merged on the left side.\n",
    "- right DataFrame to be merged on the right side.\n",
    "- how One of 'inner', 'outer', 'left', or 'right'; defaults to 'inner'.\n",
    "- on Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys \n",
    "    given, will use the intersection of the column names in left and right as the join keys.\n",
    "- left_on Columns in left DataFrame to use as join keys.\n",
    "- right_on Analogous to left_on for left DataFrame.\n",
    "- left_index Use row index in left as its join key (or keys, if a MultiIndex).\n",
    "- right_index Analogous to left_index.\n",
    "- sort Sort merged data lexicographically by join keys; True by default (disable to get better performance in\n",
    "    some cases on large datasets).\n",
    "- suffixes Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y') (e.g., if  'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result).\n",
    "- copy If False, avoid copying data into resulting data structure in some exceptional cases; by default always copies.\n",
    "- indicator Adds a special column _merge that indicates the source of each row; values will be 'left_only',\n",
    "     'right_only', or 'both' based on the origin of the joined data in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data\n",
    "- https://datascience.stackexchange.com/questions/21650/feature-transformation-on-input-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic type\n",
    "\n",
    "- float format:\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray\n",
    "- ravel vs flatten\n",
    "    - https://stackoverflow.com/questions/28930465/what-is-the-difference-between-flatten-and-ravel-functions-in-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.pipeline\n",
    "- it always apply on data transformation stage rather than model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tslearn.preprocessing\n",
    "- tslearn.preprocessing.TimeSeriesScalerMinMax(min=0.0, max=1.0)\n",
    "    - Scaler for time series. Scales time series so that their span in each dimension is between min and max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.preprocessing\n",
    "- sklearn.preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1, 2])\n",
    "arr = lb.transform([1, 2])\n",
    "arr.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "- https://machinelearningmastery.com/normalize-standardize-time-series-data-python/\n",
    "- Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "\n",
    "```\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "df_scalar = df.loc[\"only scalar can be normalized\"]\n",
    "ndarray_normalized = min_max_scaler.fit_transform(df_scalar)\n",
    "df_normalized = pd.Dataframe(data = ndarray_normalized,index = df_scalar.index,columns = df_scalar.columns)\n",
    "\n",
    "```\n",
    "- http://www.dataminingblog.com/standardization-vs-normalization/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Tips:\n",
    "    - when learning a new model, start from the fit function.\n",
    "    - According with the paper, check the algorithm step by step.\n",
    "    - Compare different notation in paper and code.\n",
    "- generative model:\n",
    "    - generative classifiers (joint distribution)\n",
    "- discriminative model (conditional distribution or no distribution)\n",
    "- A taxonomy: These categories include unsupervised, semi-supervised, supervised, and reinforcement learning\n",
    "- several views of a model:\n",
    "    - assumptions\n",
    "    - for what kind problems:\n",
    "    - main steps\n",
    "    - effiency\n",
    "        - Fitting time on train set\n",
    "        - Query time for singel instances in test set\n",
    "    - accuracy\n",
    "    - interpretability: explain the classifier decision\n",
    "    - stability\n",
    "    - generalizability :which is defined as a model’s ability to fit current data but also to predict future data!\n",
    "    - separated\n",
    "- Statics vs ML model\n",
    "    - https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3\n",
    "    \n",
    "- algorithm vs model\n",
    "- https://artificialneuralnetworks.org/difference-between-algorithm-and-model-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Modeling Technique\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "- 各种算法优缺点比较\n",
    "    - https://static.coggle.it/diagram/WHeBqDIrJRk-kDDY/t/categories-of-algorithms-non-exhaustive\n",
    "   \n",
    "- 要注意各种模型的使用前提和假设，使用算法前首先验证变量是否满足假设。\n",
    "- Two Goals:\n",
    "    - Complex enough to fit the data well\n",
    "    - Simple to interpret, does not overfit the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Square Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Y = b_0 + b_1 X\n",
    "\\\\\n",
    "b_1 = \\frac {\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(x_i - \\bar{X})^2}\n",
    "\\\\\n",
    "b_0 = \\bar{Y} - b_1 \\bar{X}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression\n",
    "- supervised learning\n",
    "- the assumptions before use regression\n",
    "- https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/\n",
    "    - Regression is a parametric approach. ‘Parametric’ means it makes assumptions about data for the purpose of analysis.\n",
    "    - There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). \n",
    "    - There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.\n",
    "    - The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.\n",
    "    - The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.\n",
    "    - The error terms must be normally distributed.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[1,1],[1,2],[2,2],[2,3]])\n",
    "#y = np.dot(X,np.array([1,2])) + 3\n",
    "\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "reg = LinearRegression().fit(x1[:,np.newaxis],x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8,  9, 11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000000000000002"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 1.5, 2.5, 2.5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(x1[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(x1,x2)\n",
    "plt.plot(reg.predict(x1[:,np.newaxis]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive model(AM)\n",
    "- nonparametric regression\n",
    "- Suits if the size of seasonal fluctuations (or variation around the trend Tt) doesn’t vary with the time series level\n",
    "- Means the seasonality is the same (roughly constant) in same period over different years (does not depend on level)\n",
    "-  more interpretable than a general regression surface at the cost of approximation errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative Model\n",
    "- Suits if the variation in seasonal pattern or that around the trend (Tt) does vary with the time series level.\n",
    "- Sometimes seasonal effect is a proportion of underlying trend value, e.g. in previous slide, they increase with trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive\n",
    "- all forecasts for the future are equal to the last observed value of the series\n",
    "\\begin{equation*}\n",
    "y_t = y_{t-1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "- https://facebook.github.io/prophet/\n",
    "- https://research.fb.com/prophet-forecasting-at-scale/\n",
    "- https://pbpython.com/prophet-overview.html\n",
    "- https://colab.research.google.com/drive/1dL_XNqTg6KGjuGhSNZkajvFZQMNjcc9H?authuser=1#scrollTo=9E1nB_64XVrB\n",
    "- can not install on windows10, has a known problem.\n",
    "    - we can use colab.\n",
    "\n",
    "- Title: Forecasting at Scale\n",
    "- Review Date: 2019-3-19 12:42:302\n",
    "- Published year: 27 Sep 2017\n",
    "- Auther: Facebook\n",
    "- Unicode:\n",
    "- Useful Reference:\n",
    "- Usefulness:\n",
    "- Expression:\n",
    "    - pronounced dip around Christmas and New Year.\n",
    "    - is no exception\n",
    "    - intuitive parameters\n",
    "    - inherently different from\n",
    "- Key Words:\n",
    "    - three type scale:\n",
    "        - people\n",
    "        - variety of forecasting problems\n",
    "        - forecasting model evaluation\n",
    "- Purpose Problem: \n",
    "    - two main forecast themes:\n",
    "        - Completely automatic forecasting techniques can be brittle and they are often too inflexible to incorporate useful assumptions or heuristics.\n",
    "        - Analysts who can produce high quality forecasts are quite rare because forecasting is a specialized data science skill requiring substantial experience.\n",
    "    - a practical approach to forecasting \\at scale\" that combines configurable models with analyst-in-the-loop performance analysis\n",
    "    - high quality forecasts often far outstrips the pace at which they can be produced.\n",
    "    - Prophet’s features, \n",
    "        - like multiple seasonality\n",
    "        - changing growth rates\n",
    "        - the ability to model special days (such as Manning’s playoff and superbowl appearances)\n",
    "- Algorithm:\n",
    "    - why prophet? 6 point       \n",
    "        - hourly, daily, or weekly observations with at least a few months (preferably a year) of history\n",
    "        - more modeling flexibility,\n",
    "    - HOW prophtet work?\n",
    "        - A piecewise linear or logistic growth curve trend.\n",
    "        - A yearly seasonal component modeled using Fourier series.\n",
    "        - A weekly seasonal component using dummy variables.\n",
    "        - A user-provided list of important holidays.\n",
    "        - hundred HMC iterations\n",
    "        - We fit the Prophet model using Stan,\n",
    "            - Stan® is a state-of-the-art platform for statistical modeling and high-performance statistical computation.\n",
    "\n",
    "- Features of Business Time Series\n",
    "    - seasonality: weekly, yearly\n",
    "    - trend\n",
    "    - outliers\n",
    "    - problem of automated methods:\n",
    "        - auto.arima: large trend errors\n",
    "        - exponential smoothing : missing long-term seasonality\n",
    "        - overreact to the end of year dip.\n",
    "        - Tuning these methods requires a thorough understanding of how the underlying time series models work.\n",
    "- The prophet forecasting model\n",
    "    - handle the common features above\n",
    "    - intuitive parameters \n",
    "    - a decomposable model\n",
    "\\begin{equation*}\n",
    "y(t) = g(t) + s(t) + h(t) + \\xi_t\n",
    "\\end{equation*}\n",
    "        - g(t): trend function\n",
    "        - s(t): periodic changes，weekly and yearly seasonality\n",
    "        - h(t): effects of holidays\n",
    "        - \\xi_t: idiosyncratic changes\n",
    "        - [ ]similar to GAM\n",
    "            - decomposes easily and accommodates new components as necessary\n",
    "     - Several practical advantages:\n",
    "         - flexibility\n",
    "         - not need to be regularly spaced, not need to interpolate missing values\n",
    "\n",
    "#### The Trend Model:\n",
    "- Prophet detects changepoints by first specifying a large number of potential changepoints at which the rate is allowed to change. It then puts a sparse prior on the magnitudes of the rate changes (equivalent to L1 regularization) - this essentially means that Prophet has a large number of possible places where the rate can change, but will use as few of them as possible.\n",
    "- Logistic growth model/nonlinear, saturating growth\n",
    "\n",
    "\\begin{equation*}\n",
    "g(t) = \\frac{C}{1+exp(-k(t-m))} \n",
    "\\end{equation*}\n",
    "\n",
    "    - C: carring capacity\n",
    "    - k: growth rate\n",
    "    - m : an offset parameter? How to understand this\n",
    "\n",
    "- piecewise logistic growth model\n",
    "    - for trend changes\n",
    "- piecewise constant rate growth model for forecasting.\n",
    "\\begin{equation*}\n",
    "g(t) = (k + a(t)^T\\delta)t + (m + a(t)^T\\gamma) \n",
    "\\end{equation*}  \n",
    "- automatic changepoint selection\n",
    "    - MLE\n",
    "    - [ ]a sparse prior\n",
    "\n",
    "#### Seasonality\n",
    "- standard Fourier series\n",
    "- partial Fourier sum\n",
    "\\begin{equation*}\n",
    "s(t) = \\sum (a_n\\cos(\\frac{2\\pi nt}{P}) + b_n\\sin(\\frac{2\\pi nt}{P}))\n",
    "\\end{equation*}\n",
    "\n",
    "#### Holidays and Events\n",
    "- allow the analyst to provide a custom list of past and future events, identified by the event or holiday's unique name\n",
    "\\begin{equation*}\n",
    "h(t) = Z(t)k\n",
    "\\\\\n",
    "Z(t) = [1(t \\in D_1),...,1(t \\in D_t)] \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "#### Model Fitting\n",
    "- regularization\n",
    "    - https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2\n",
    "delta \u0018 double_exponential(0, tau);\n",
    "beta \u0018 normal(0, sigma);\n",
    "\n",
    "#### Automating Evaluation of Forecasts\n",
    "\n",
    "- Modeling Forecast Accuracy\n",
    "    - MAPE for interpretability : Mean absolute percentage error\n",
    "    \n",
    "\n",
    "***\n",
    "- Experiment:\n",
    "- Conclusion:\n",
    "- Limitation:\n",
    "- Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### impletation\n",
    "\n",
    "> https://facebook.github.io/prophet/docs/quick_start.html#python-api\n",
    "- The input to Prophet is always a dataframe with two columns: ds and y\n",
    "    - ds: datestamp,  format expected by Pandas\n",
    "    - y: The y column must be numeric, and represents the measurement we wish to forecast.\n",
    "- Automatic changepoint detection in Prophet\n",
    "    - 1. specifying a large number of potential changepoints at which the rate is allowed to change\n",
    "    - 2. puts a sparse prior on the magnitudes of the rate changes : same as L1 regularization\n",
    "        - L1 regularization: https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2\n",
    "        - Loss function is the sum of squared difference between the actual value and the predicted value\n",
    "        - by penalizing the loss function, regularization discourage the complexity of the model\n",
    "        - Regularization works on assumption that smaller weights generate simpler model and thus helps avoid overfitting.\n",
    "        - L1 regularization does feature selection. It does this by assigning insignificant input features with zero weight and useful features with a non zero weight.\n",
    "    - 3. will use as few of them as possible\n",
    "    - by increasing changepoint_prior_scale, will increase the forecast uncertainty\n",
    "\n",
    "\n",
    "\n",
    "- Holidays\n",
    "    - 1. create a dataframe for holidays\n",
    "\n",
    "- Seasonalities \n",
    "    - are estimated using a partial Fourier sum\n",
    "    - Prophet will by default fit weekly and yearly seasonalities\n",
    "    - You can add other seasonalities (monthly, quarterly, hourly) using the add_seasonality method (Python)\n",
    "    - seasonality_prior_scale which similarly adjusts the extent to which the seasonality model will fit the data.\n",
    "    - By default Prophet fits additive seasonalities, meaning the effect of the seasonality is added to the trend to get the forecast. \n",
    "    - grows with the trend. This is multiplicative seasonality.\n",
    "\n",
    "- Uncertainty Intervals\n",
    "    - There are three sources of uncertainty in the forecast: uncertainty in the trend, uncertainty in the seasonality estimates, and additional observation noise.\n",
    "    - The biggest source of uncertainty in the forecast is the potential for future trend changes.\n",
    "    - by increasing changepoint_prior_scale, will increase the forecast uncertainty\n",
    "    - To get uncertainty in seasonality, you must do full Bayesian sampling. This is done using the parameter mcmc.samples (which defaults to 0)\n",
    "\n",
    "- outliers\n",
    "    -  Prophet is able to handle the outliers in the history, but only by fitting them with trend changes.\n",
    "    - The best way to handle outliers is to remove them\n",
    "\n",
    "\n",
    "\n",
    "- Data with regular gaps\n",
    "    -  fit a daily cycle to a time series that only has data for part of the day (12a to 6a)\n",
    "    - The solution is to only make predictions for the time windows for which there are historical data.\n",
    "\n",
    "- Diagnostics : This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "> key steps:\n",
    "##### step1:  import Package\n",
    "\n",
    "```\n",
    "from fbprophet import Prophet\n",
    "```\n",
    "\n",
    "\n",
    "##### step2:  a new Prophet object : Any settings to the forecasting procedure are passed into the constructor\n",
    "\n",
    "```\n",
    "playoffs = pd.DataFrame({\n",
    "  'holiday': 'playoff',\n",
    "  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n",
    "                        '2010-01-24', '2010-02-07', '2011-01-08',\n",
    "                        '2013-01-12', '2014-01-12', '2014-01-19',\n",
    "                        '2014-02-02', '2015-01-11', '2016-01-17',\n",
    "                        '2016-01-24', '2016-02-07']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "superbowls = pd.DataFrame({\n",
    "  'holiday': 'superbowl',\n",
    "  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "holidays = pd.concat((playoffs, superbowls))\n",
    "\n",
    "\n",
    "\n",
    "m = Prophet(changepoint_range=0.9,\n",
    "        changepoint_prior_scale=0.5,\n",
    "        changepoints=['2014-01-01'],\n",
    "        holidays = holidays,\n",
    "        yearly.seasonality = 20,\n",
    "        weekly_seasonality=False,\n",
    "        seasonality_mode='multiplicative')\n",
    "```\n",
    "- trend adjust\n",
    "    - n_changepoints\n",
    "        - number of potential changepoints\n",
    "        - By default, Prophet specifies 25 potential changepoints which are uniformly placed in the first 80% of the time series\n",
    "    - changepoint_range=0.9\n",
    "        - By default changepoints are only inferred for the first 80% \n",
    "    - changepoint_prior_scale=0.5:\n",
    "        - Adjusting trend flexibility, overfit (too much flexibility) or underfit (not enough flexibility)\n",
    "        - By default, this parameter is set to 0.05.  Increasing it will make the trend more flexible\n",
    "    - changepoints=['2014-01-01']\n",
    "        - changepoints manually\n",
    "\n",
    "- seasonality adjust:\n",
    "    - seasonality_prior_scale : default 10, Larger values allow the model to fit larger seasonal fluctuations, smaller values dampen the seasonality. \n",
    "\n",
    "- Holiday\n",
    "- holidays_prior_scale : default 10, same as seasonality\n",
    "\n",
    "- yearly.seasonality = 20\n",
    "    - Fourier order can be specified for each built-in seasonality\n",
    "- seasonality_mode:\n",
    "\n",
    "```\n",
    "\n",
    "# add Built-in Country Holidays\n",
    "m.add_country_holidays(country_name='US')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# add add other seasonalities (monthly, quarterly, hourly) \n",
    "# besides the default weekly and yearly seasonalities\n",
    "m.add_seasonality(\n",
    "    name='weekly', period=7, fourier_order=3, prior_scale=0.1)\n",
    "\n",
    "```\n",
    "- prior_scale=0.1: \n",
    "    - Prior scales can be set separately for individual holidays by including a column prior_scale in the holidays dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- holidays = holidays\n",
    "    - two columns (holiday and ds) and a row for each occurrence of the holiday. \n",
    "    - The holiday effect can be seen in the forecast dataframe(see s4) \n",
    "    - The holiday effects will also show up in the components plot(see s5)\n",
    "\n",
    "\n",
    "```\n",
    "# add extra regressor\n",
    "def nfl_sunday(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.weekday() == 6 and (date.month > 8 or date.month < 2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['nfl_sunday'] = df['ds'].apply(nfl_sunday)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = Prophet()\n",
    "m.add_regressor('nfl_sunday')\n",
    "```\n",
    "- Additional regressors can be added to the linear part of the model using the add_regressor method or function\n",
    "- regressors must be added prior to model fitting.\n",
    "\n",
    "```\n",
    "m\n",
    "```\n",
    "\n",
    "##### step3: make a datafrmae include the future date you want to forecasting.\n",
    "```\n",
    "#Prophet.make_future_dataframe\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "\n",
    "# only predict monthly data\n",
    "#future = m.make_future_dataframe(periods=120, freq='M')\n",
    "\n",
    "future.tail()\n",
    "```\n",
    "##### step4: Predict: The predict method will assign each row in future a predicted value which it names yhat.\n",
    "    - The predict method will assign each row in future a predicted value which it names yhat.\n",
    "```\n",
    "\n",
    "forecast = m.fit(df).predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "# holiday effect\n",
    "forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][\n",
    "        ['ds', 'playoff', 'superbowl']][-10:]\n",
    "```\n",
    "\n",
    "##### step5: Plot: plot the forecast by calling the Prophet.plot\n",
    "    - plot change points\n",
    "```\n",
    "\n",
    "# show holiday effect\n",
    "# show seasonality effect\n",
    "fig2 = m.plot_components(forecast)\n",
    "\n",
    "\n",
    "\n",
    "# show changepoints\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "fig = m.plot(forecast)\n",
    "a = add_changepoints_to_plot(fig.gca(), m, forecast)\n",
    "\n",
    "# plot_cross_validation_metric,\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mape')\n",
    "```\n",
    "\n",
    "\n",
    "##### step6: diagnostics\n",
    "\n",
    "```\n",
    "# cross_validation\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "df_cv = cross_validation(m, initial='730 days', period='180 days', horizon = '365 days')\n",
    "df_cv.head()\n",
    "\n",
    "```\n",
    "- initial: the size of the initial training period\n",
    "    - if the cutoff is less than initial, stop calculate the cutoff.\n",
    "    - default, the initial training period is set to three times the horizon\n",
    "- horizon: forecast horizon \n",
    "    -- As a heuristic, for a forecast horizon H, we generally make a simulated forecast every H/2 periods.\n",
    "- period: spacing between cutoff dates \n",
    "    - start from every cutoff date point \n",
    "    - default: cutoffs are made every half a horizon.\n",
    "- steps:\n",
    "    - s1. get the cut off point from the end of the time-series date       \n",
    "    - s2. train the model use the data from start to the first cutoff\n",
    "    - s3. predict the cutoff + horizon data\n",
    "    - s4. move to the next cutoff, repead s2,s3 until the last cutoff\n",
    "    - s5. output a sets of prediction value, ea\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# performance_metrics \n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p.head()\n",
    "```\n",
    "\n",
    "```\n",
    "# plot the performace metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mape')\n",
    "```\n",
    "-  The default is 0.1, corresponding to 10% of rows from df_cv\n",
    "\n",
    "\n",
    "- uncertainty interval\n",
    "    - By default Prophet will only return uncertainty in the trend and observation noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# holiday example\n",
    "playoffs = pd.DataFrame({\n",
    "  'holiday': 'playoff',\n",
    "  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n",
    "                        '2010-01-24', '2010-02-07', '2011-01-08',\n",
    "                        '2013-01-12', '2014-01-12', '2014-01-19',\n",
    "                        '2014-02-02', '2015-01-11', '2016-01-17',\n",
    "                        '2016-01-24', '2016-02-07']),\n",
    "  #'lower_window': 0,\n",
    "  #'upper_window': 1,\n",
    "})\n",
    "superbowls = pd.DataFrame({\n",
    "  'holiday': 'superbowl',\n",
    "  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "holidays = pd.concat((playoffs, superbowls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive and Random Walk\n",
    "- the same model, different names.\n",
    "\\begin{equation*}\n",
    "y_t = y_{t-1} + e_t \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression(AR)\n",
    "> https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/\n",
    "- the only model can use KFold cross validation\n",
    "\n",
    "\\begin{equation*}\n",
    "y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + e_t \n",
    "\\end{equation*}\n",
    "\n",
    "- et: whiti noise(WN)\n",
    "- is a multiple regression\n",
    "- y_t is the lagged value as predictions\n",
    "- if p= 1 : AR(1) model\n",
    "\n",
    "\\begin{equation*}\n",
    "y_t = c + \\phi_1 y_{t-1} + e_t \n",
    "\\end{equation*}\n",
    "\n",
    "- if \\phi_1 =0: == WN\n",
    "- if \\phi_1 = 1 and c=0,y_t = RW(Random Walk)\n",
    "\\begin{equation*}\n",
    "y_t = y_{t-1} + e_t \n",
    "\\end{equation*}\n",
    "- 0 \n",
    "    - RW model is where current value is previous plus random step up or down\n",
    "    - RW has zero mean but time-varying variance, thus is non-stationary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.68532718]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "from random import random\n",
    "\n",
    "# contrived dataset\n",
    "data = [x + random() for x in range(1,100)]\n",
    "\n",
    "# fit model\n",
    "model = AR(data)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data),len(data))\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VNX9x/H3IQlkgSQQAoSEfUd2I0S0qLhUgYqKWjewgKDVWmtrW7Ta2lZb7a91rbXSIqCyiBaE1q0KuIEsYSdsCZCEEEICSUgghGzn90cGS22QJLNl7nxez5MnMzd35n4vd/jk5Nx7zzHWWkRExLma+bsAERHxLgW9iIjDKehFRBxOQS8i4nAKehERh1PQi4g4nIJeRMThFPQiIg6noBcRcbhQfxcA0LZtW9u1a1d/lyEiElA2bNhwxFobf671mkTQd+3aldTUVH+XISISUIwxWfVZT103IiIOp6AXEXE4Bb2IiMOdM+iNMa8aY/KNMdvPWNbGGPORMSbd9b21a7kxxrxgjMkwxmw1xgzzZvEiInJu9WnRzwGu/tqyGcBya20vYLnrOcA1QC/X13TgZc+UKSIijXXOoLfWfgYUfm3xeGCu6/Fc4Lozlr9ma60BYo0xCZ4qVkREGq6xffTtrbWHAFzf27mWJwIHzlgvx7VMRET8xNMnY00dy+qcq9AYM90Yk2qMSS0oKPBwGSIiTUtVdQ2pmYXMWbWfE6eqfLrtxt4wddgYk2CtPeTqmsl3Lc8BOp2xXhKQW9cbWGtnAjMBkpOTNXGtiDjSweKTPP3+Lj7ZnU9JeW3Ax7VswXcGd/RZDY1t0S8D7nQ9vhNYesbySa6rb1KAY6e7eEREgs3afUe59sUvWLErn2+f14Enrx8AQPHJSp/Wcc4WvTFmAXAp0NYYkwP8CngKWGSMmQpkAze5Vn8PGANkAGXAZC/ULCLS5L2xJovHl6XRuU0kMycl07NdS8orq/nFku2UNLWgt9beepYfXV7Huha4z92iREQC2XMf7+G5j9O5rE88z90ylJiIMADCw0JoHtqMkvImFvQiIlJ/f/10L899nM6N5yfx9IRBhDT772tUosNDKTnp25OxGgJBRMRD5q7O5Kn3d/GdwR3rDHmA6PAwtehFRAJFTlEZr6/JIvPICbKOlrErr5Sr+rfnmZsH1xnyAK0iwppeH72IiPyvwyXl3DJzDYdLyukaF0WXuCi+fV4H7r2sB2EhZ+8siQ4P/eoyS19R0IuINFBxWQWTZq2j6EQF//j+SAYlxdb7tdERYRwsPunF6v6Xgl5EpAHKKqqYMmc9+4+cYPbkCxoU8uDqo/fxyVgFvYhIPeQdK2f+2izmrztA4YlT/OX2YVzUs22D3yc6IlQnY0VEmpKq6hqe/mAXs1dlUm0to/u0Y9qo7qR0j2vU+0WHh1FRVUN5ZTXhYSEerrZuCnoRkbM4VlbJDxZs5PP0I9w6vBPfv6QnneMi3XrP6PDa2C0pr1TQi4j40/aDx7h/wSZyisr4w4RB3HxBp3O/qB6iXXfJlpysol0rj7zlOSnoRURcCk9UsGTTQRZvzCEtt4S4qObMn5bCBV3beGwb0eGuoPdhP72CXkSCXk2NZf66bJ7+YBel5VUMSorh8e/0Z/yQRFpHNffotqIjamO31IfX0ivoRSSo7cor4eHF29iUXczIHnE8Nq4//RKivba9r1r0Prw7VkEvIkHrw7Q8Hli4icjmoTxz82CuH5qIMXUPXeApX/XRq+tGRMS7Zn2xnyfe3cHgpFj+NimZ+FYtfLLd/7To1XUjIuIV5ZXVPPnuTl5fk8XV53Xg2e8OIaK5by5zBAgPa0ZYiFGLXkTEGzZkFfHTt7ewr+AE00d1Z8bVfWl2llEmvcUYQ6tw345gqaAXEccrKa/kxeXpzPpiPwkxEbwxdQQX92r48AWe4usRLBX0IuJYFVU1vLEmixdXpFNUVsmtwzvzyJi+tHL1k/tLtI/HpFfQi4gjrco4wiNLtpF1tIyLesbx8DX9GJAY4++ygNoTsqXqoxcRaZyS8kp+/95OFqw7QLe2UcyZfAGX9I73+mWTDREdEUpeSbnPtqegFxHHyMg/zqRZa8krKefuUd158MrePhs4rCGidTJWRKTh9h85wW1/W0ONhcX3XsSQTg2bEMSXoiN8O0G4gl5EAt6BwjJu+9saqmosC6en0Lu9j4aFbKRWLUIpr6zhVFU1LUK9/xfH2WewFREJABuyirhl5hrKKqp5Y+qIJh/y8J9hEHw1sJla9CISkI4cP8XT7+/irQ05dIgO5/Wpw+nf0XuDkXnS6REsS05W0ral94deUNCLSMCorrGszyzkX1tzWbo5l5MV1dx9SXd+OLoXUS0CJ87+Mya9WvQiIgBkHT3BwvUHWLwxh8Mlp4gIC+Hyfu340RW96OmraZo86D9dN745IaugF5Ema+ehEp54dwerMo4S0sxwWZ94Hh2byOX92hHZPHDjy9cjWAbuv5SIONqqjCPc/foGwsNC+MmVvbkpuRMdYsL9XZZHfNVHrxa9iASrdzYd5Kdvb6F725bMmXIBCTER/i7Jo3w9y5SCXkSajJoay4srMnj24z2kdG/DKxOTiYnw7wBk3hDZPISQZr4bk96toDfGPAjcBVhgGzAZSAAWAm2AjcBEa22Fm3WKiMMVnajgwUWb+WR3ATcMTeT3Ewb65GYif6gdkz7UZ330jb5hyhiTCPwQSLbWDgBCgFuAp4FnrbW9gCJgqicKFRHn2n7wGONe/ILVGUd54roB/OnmwY4N+dOiw303DIK7d8aGAhHGmFAgEjgEjAbedv18LnCdm9sQEQf7dE8BN7/yJdZaFt1zIXekdGlSI016S3REqM/66Bsd9Nbag8AfgWxqA/4YsAEottae/nskB0is6/XGmOnGmFRjTGpBQUFjyxCRALZ4Yw5T56ynS1wUS+5r2gOReVrtmPRNv+umNTAe6AZ0BKKAa+pY1db1emvtTGttsrU2OT4+vrFliEgAOlh8kseXpfHjRVsY3q0Nb96dQvtoZ1w6WV++7Lpx52TsFcB+a20BgDFmMTASiDXGhLpa9UlArvtlikigs9aSmlXEnFWZfJCWB8Ctwzvx+LXnOb4/vi61XTdN/4apbCDFGBMJnAQuB1KBlcCN1F55cyew1N0iRSRwHTtZyeKNOSxYl82ew8dpFR7KXRd3Y9LIriTGOuv6+IZoFQgtemvtWmPM29ReQlkFbAJmAu8CC40xT7iWzfJEoSISWKy1LNuSy+PL0igqq2Rwp1ienjCQcYM6BtQAZN4SHR5GWUU1ldU1hIV4d8R4t/61rbW/An71tcX7gOHuvK+IBLb80nIeXbKdf+84zOBOscydch6DkoLnRGt9nB4GobS8ijZRzb26Lf1aFRGPyikqY8LLqykuq+SRMX2ZenF3Qpo5/3LJhjpzGAQFvYgEjKITFUx6dR0nK6pZcu9FATMRiD+cHqrYF/30CnoR8Yjyymruei2VnKKTvDF1hEL+HKLD/9N1422aM1ZE3FZaXsm98zayMbuI5747hOHd2vi7pCbvqxa9D+6OVYteRNyyIauQBxZuJrf4JL8dP4AxAxP8XVJAUNeNiDR5x09V8cqne3lpZQaJrSN4654LOb+LWvL11ToyjNtGdKZrXJTXt6WgF5EGOVZWyezV+5m9KpNjJyu5YWgivx5/Hq3CnTduvDdFNg/ld9cP9Mm2FPQiUi/bco4xf102yzYf5ERFNVf0a88PRvcMqoHIApWCXkS+0Y7cEh5evJUtOccID2vG2IEdmXpxN11VE0AU9CJyVovWH+CxpduJiQjjN+PPY/yQREdO7ed0CnoR+R/FZRU8+e5O3tqQw8gecTx/y1DiW7Xwd1nSSAp6EflKcVkFr35Re6L1eEUV94/uyY+u6K0hDAKcgl5EAPgwLY+HFm2h9FQVYwZ24P7RveiXoH54J1DQiwiL1h9gxuKtDEqK5akJA+nbQQHvJAp6kSD310/38tT7uxjVO56/3jGMyOaKBafRERUJUuWV1fz6nztYsC6bcYMSeObmITQP1fBXTqSgFwlC2UfL+P68DaTllvD9S3vw0FV9dMLVwRT0IkHmg+15/PTtLRjg75OSuaJ/e3+XJF6moBcJEuWV1Tzx7g7eWJPNoKQYXrptGJ3aRPq7LPEBBb1IENh8oJifvb2FPYePM31Udx66qo/644OIgl7EwVIzC3lhRQaf7SmgbcsWzJ0ynEt6x/u7LPExBb2Iw9TUWFbuzueVz/axbn8hcVHNmXFNX+5I6ULLFvovH4x01EUcwlrLe9vyePbjPWTkH6djTDiPjevPbcM7E9E8xN/liR8p6EUcoKD0FI+9s50P0vLo26EVz313CGMHJRAWon54UdCLBLxlW3L55dLtlFVU8/A1fZl6cTdCFfByBgW9SIAqLqvg0Xe286+thxjaOZb/u3EwPdu19HdZ0gQp6EUC0BfpR/jJW5s5eryCn367D3eP6q5WvJyVgl4kwCxcl80jS7bRI74ls+68gAGJMf4uSZo4Bb1IgLDW8vzydJ77OJ1Lesfzl9uHEaXLJaUe9CkRCQCl5ZX85p87eGtDDhOGJfHUhIG6okbqTUEv0oRZa/kwLY9fLUsjv/QU94/uyY+v7I0xGmlS6s+toDfGxAJ/BwYAFpgC7AbeBLoCmcDN1toit6oUCSKl5ZVszTnG5gPFfJF+hC/3HaVfQjSvTExmSKdYf5cnAcjdFv3zwAfW2huNMc2BSOARYLm19iljzAxgBvBzN7cj4nhV1TXMXpXJnz7aTXllDQDd20bx6Nh+fG9kV11VI43W6KA3xkQDo4DvAVhrK4AKY8x44FLXanOBT1DQi3yjHbklzFi8la05x7iiXzsmXtiVwUkxxEY293dp4gDutOi7AwXAbGPMYGAD8ADQ3lp7CMBae8gY0879MkWca+nmgzz01hZiIsL4821DGTswQX3w4lHu/C0YCgwDXrbWDgVOUNtNUy/GmOnGmFRjTGpBQYEbZYgErte/zORHb25maOfWfPTgJYwb1FEhLx7nTtDnADnW2rWu529TG/yHjTEJAK7v+XW92Fo701qbbK1Njo/X+NgSXKy1vLg8nceWpnF533a8NmU4raPUTSPe0eigt9bmAQeMMX1ciy4HdgDLgDtdy+4ElrpVoYjDWGv5w4e7+dNHe7hhaCIv33E+4WEaRli8x92rbu4H5rmuuNkHTKb2l8ciY8xUIBu4yc1tiDiGtZbfvbeTv32+n9tHdOa34wfQrJm6asS73Ap6a+1mILmOH13uzvuKOFF1jeWJd3cwe1Umd17YhcevPU/98eITujNWxMtKyytZlJrD3NWZZBeWMeWibjw2rp9CXnxGQS/iBeWV1XyefoT3tx3iw7Q8TlRUk9ylNQ9f05erB3RQyItPKehFPGzJphweeyeN46eqiIkIY+ygBO5I6cKgJA1fIP6hoBfxoA/T8njora2c37k1943uycgecRplUvxOQS/iIaszjnD//E0MTIxh9uQLNFa8NBn6JIq4yVrLil35/HDBJrq1jWKOQl6aGH0aRRrJWsvqvUd55qM9bMgqont8FK9NHa6ByKTJUdCLNEJ5ZTU/e3sry7bkkhATzpPXD+Cm8zvRPFT98dL0KOhFGqjwRAXTX0slNauIB6/ozd2XdNcQBtKkKehFGiAj/zjTXkvlYPFJ/nzbUMYN6ujvkkTOSUEvUg95x8p5YUU6i9YfoFV4KPPvGkFy1zb+LkukXhT0It+gsrqGF5en88pn+6ixlluHd+b+0T1pFx3u79JE6k1BL3IWBwrLeGDhJjZmFzN+SEceuqoPndpE+rsskQZT0It8jbWWf209xC+WbKPGwgu3DuXaweqLl8CloBc5w96C4zy+LI3P048wOCmGF28dRuc4teIlsCnoRagdK/755em8/EkG4WEhPP6d/tyR0oVQjVMjDqCgl6BXWl7JAws3s2JXPtcPTeSRMf2Ib9XC32WJeIyCXoJa9tEy7nptPXsLTvDEdQO4I6WLv0sS8TgFvQSl/JJyZq3az7w12YQ0M7w+ZTgje7b1d1kiXqGgl6CSW3ySl1Zm8NaGHKqqaxg7qCMPXdWbLnFR/i5NxGsU9BIU8kvL+cvKvcxfmw3AjclJ3D2quwJegoKCXhxvxa7D/GD+Jk5V1XBzchI/GN2LxNgIf5cl4jMKenG0N9dn88iS7fRLaMWfbx1G17ZqwUvwUdCLI9XUWF5ckcGzH+9hVO94/nL7MFpq1icJUvrki+Pszivl0Xe2sT6ziBuGJfL0hEGaoFuCmoJeHKOsoooXlmfw98/30So8lD9MGMRNyUkYY/xdmohfKegl4Flr+TAtj9/8cwe5x8q56fwkHh7TjzZRmrtVBBT0EuAOHTvJjH9s49M9BfTt0Irnbx3KBZoQROS/KOglYK3PLOT7b2zgZEU1vxzXn0kXahAykboo6CUgzVubxePL0khqHcmCaSn0at/K3yWJNFkKegkohScq+NWyNP65JZdL+8Tz/C1DiYkI83dZIk2a20FvjAkBUoGD1tpxxphuwEKgDbARmGitrXB3OyIfbM/j0Xe2cexkJT+5sjf3XtaTkGa6okbkXDzRon8A2AlEu54/DTxrrV1ojPkrMBV42QPbkSBTXlnN5+lHWJVR+5Wef5zzOkbz+tQR9EuIPvcbiAjgZtAbY5KAscCTwI9N7QXLo4HbXKvMBR5HQS8NcKqqmgVrs3npk70UlJ4iPKwZF3Rtwx0pXbhtRGfd/CTSQO626J8DfgacPhMWBxRba6tcz3OARDe3IUHCWsvijQf50793k3usnBHd2vDHmwaT0r0NLUJD/F2eSMBqdNAbY8YB+dbaDcaYS08vrmNVe5bXTwemA3Tu3LmxZYhDZOQf5xdLtrF2fyFDOsXyfzcNZmSPON3VKuIB7rToLwKuNcaMAcKp7aN/Dog1xoS6WvVJQG5dL7bWzgRmAiQnJ9f5y0CCw9zVmTzx7g4iwkL4/Q0D+W5yJ5rpJKuIxzS6s9Na+7C1Nsla2xW4BVhhrb0dWAnc6FrtTmCp21WKY/07LY9fLUtjVK94Vjx0KbcO76yQF/Ewb5zV+jm1J2YzqO2zn+WFbYgD7M4r5cE3NzM4KYaXbh9G25Yt/F2SiCN55IYpa+0nwCeux/uA4Z54X3Gu4rIKpr2WSmSLUF6ZmEx4mE62iniL7owVnzhVVc3rX2axMbuI3OJyso6e4MSpahbenUKHmHB/lyfiaAp68brVGUd4dOl29hWcoHvbKDrGRnB5v/ZcO7gjwzq39nd5Io6noBevOVlRzaPvbOcfG3PoEhfJnMkXcGmfdv4uSyToKOjFK/JLy5k2N5WtB49x/+ie3HdZT/XDi/iJgl48buehEqbOWU9RWSUzJyZzZf/2/i5JJKgp6MWjNmUXMWnWOiJbhPDWPRcyIDHG3yWJBD0FvXjM1pxiJr26jjYtm7NgWgodYyP8XZKI4J0bpiQIbT94jImz1hETEcZ8hbxIk6IWvbilrKKK17/M4qWVGbQKD2PBtBQSFfIiTYqCXhrlVFU1b6zJ5uVPMjhyvIJRveN58roBdGoT6e/SRORrFPTSINZa3t+ex1Pv7yK7sIyLesbx1yt6k9y1jb9LE5GzUNBLvdTUWD7Zk89fVu4lNauIPu1b8dqU4YzqHe/v0kTkHBT08o0qqmqYtzaLOaszyTpaRofocH5/w0BuTu6kiblFAoSCXs6qqrqGHy7YxAdpeSR3ac1Pv92Hb5/XQXO2igQYBb3UqbrG8uCiLXyQlscvx/VnysXd/F2SiDSSmmbyP8orq/nZ21v555ZcZlzTVyEvEuDUohegdnya+Wuz2XygmJ2HSqiqsfz4yt7cc0kPf5cmIm5S0Ae546eqeO6jPcxenUmL0GYM6RTL9FHdSekex7d6tfV3eSLiAQr6IPbl3qM8+OZm8krKuXV4Z35+dR9iI5v7uywR8TAFfZD6cu9RJs9ZR2JsBIvvHamZnkQcTEEfhNZnFjJ17no6tY5kwfQU2rZs4e+SRMSLFPRBZu2+o0yZs54OMeHMmzZCIS8SBBT0QaKquoYXV2Tw4op0usZFsWBaCu1ahfu7LBHxAQV9EDhQWMaP3tzMhqwibhiayK/Hn0er8DB/lyUiPqKgd7iVu/J5YOEmLPD8LUMYPyTR3yWJiI8p6B2qusby/PJ0XlieTv+EaP56x/l0jtNY8SLBSEHvIEeOn+KL9COkZhWyZl8hGfnHmTAsiSevH0B4WIi/yxMRP1HQO8SXe49yzxsbOHaykpYtQhnaOZZ7LunBhGGJGKPhhEWCmYLeARalHuAXS7bRuU0kc6cMZ2BijMaKF5GvKOgDWGl5Jc9+lM6rq/Zzcc+2vHT7MGIidDWNiPw3BX0AOlVVzbw12fx5ZQaFJyqYmNKFX36nvyYEEZE6NTrojTGdgNeADkANMNNa+7wxpg3wJtAVyARuttYWuV+q1NRYlm3J5Y//3k1O0UlG9ojj51f3ZXCnWH+XJiJNmDst+irgJ9bajcaYVsAGY8xHwPeA5dbap4wxM4AZwM/dLzW4fZF+hCff28nOQyWc1zGa310/kG/1aqsTrSJyTo0OemvtIeCQ63GpMWYnkAiMBy51rTYX+AQFvVve3XqI++ZvpFObCJ6/ZQjfGdSRZjrZKiL15JE+emNMV2AosBZo7/olgLX2kDGmnSe2Eaw2ZBXy4KLNnN+lNfPuGqHr4UWkwdw+e2eMaQn8A/iRtbakAa+bboxJNcakFhQUuFuGI2UeOcG01zbQMSacv01KVsiLSKO41aI3xoRRG/LzrLWLXYsPG2MSXK35BCC/rtdaa2cCMwGSk5OtO3U4hbWW5Tvz2X24lAOFZXy2pwBrLbMnD6dNlGZ+EpHGceeqGwPMAnZaa58540fLgDuBp1zfl7pVYRB5YXkGz368B4C2LZvTJS6KX4ztR7e2UX6uTEQCmTst+ouAicA2Y8xm17JHqA34RcaYqUA2cJN7JQaH97cd4tmP93DDsESeuG4Akc11i4OIeIY7V918AZzt0o/LG/u+wSgt9xg/XrSFoZ1j+d31A9UXLyIepVsp/Sz9cCnT5qYSGxnGKxPPV8iLiMepf8BPyiur+fOKDF75bC9RLUJ5Y+oITe0nIl6hoPcxay0fpuXx+/d3kXW0jBuGJvLI2H6apFtEvEZB7yPWWlbvPcofPtzNlgPF9IiPYv5dIxjZs62/SxMRh1PQe1lu8UmWbs7lnU0H2X24lI4x4fxhwiBuGJZIqEabFBEfUNB7SWV1Db97bydzVmdiLZzfpTVPXj+ACcOSdMJVRHxKQe8FhScquHfeBtbsK2RiShfu+lY3usTppicR8Q8FvYdtzC7i/vmbKDh+imduHswNw5L8XZKIBDkFvYccKCzj6Q928a+th0iICeetuy/UhCAi0iQo6N1kreUvn+zl+Y/TCWlm+OHlvbh7VHeiWuifVkSaBqWRGyqra3h0yXbeTD3A2EEJ/HJcf9pH66YnEWlaFPSNdOJUFffN38gnuwu4f3RPfnxlb03rJyJNkoK+gXbllfCPDTm8szmXo8dP8eT1A7h9RBd/lyUiclYK+no6fqqK++Zt5NM9BYQ2M4zu247JF3Xjwh5x/i5NROQbKejrofBEBd+bvY603BJmXNOXm5M7acYnEQkYCvpzyDtWzsRZa8kqLOOVO87niv7t/V2SiEiDKOjP4mDxSV7/MouF67OprKph7uTh6qYRkYCkoP+a0zc+vbftEABX9e/AA1f0ol9CtJ8rExFpHAW9y8mKal7+dC+vfLqXZsYwbVR3JqZ0Ial1pL9LExFxS9AHfXWNZcmmgzzz793kHivn2sEdeXhMXxJiIvxdmoiIRwR10K/clc9T7+9i9+FSBibG8Ox3hzCiu/rhRcRZgjLoyyur+fU/d7BgXTZd4yL5821DGTMggWbNdGeriDhP0AX93oLj3DdvI7vySrnnkh785KrehGmmJxFxsKAK+rX7jjJlznqahzZj9uQLuKxPO3+XJCLidUET9Ov2FzJ5zno6xkbw+tThOtkqIkEjKII+NbOQ781eR0JMOPOnjaBdKw0lLCLBw7FBX1FVw5f7jvLvtDyWbDpIh+hwFkxLUciLSNBxXNCXlFfy98/2MWd1JiXlVUQ2D+Gyvu14bGx/2mlSEBEJQo4J+oqqGmav2s/Ln+6luKySawZ0YMKwJC7u1ZbwsBB/lyci4jeOCPqaGsuDb27m3W2HuKR3PD/9dh8GJMb4uywRkSYh4IPeWstv393Bu9sO8ciYvkwf1cPfJYmINCkBf6fQ3z7fx+xVmUy5qBvTvtXd3+WIiDQ5Xgl6Y8zVxpjdxpgMY8wMb2wDYOnmg/zuvV2MHZTAo2P7aXJuEZE6eDzojTEhwEvANUB/4FZjTH9PbwegfXQ4V/Zvz59uGqxxakREzsIbffTDgQxr7T4AY8xCYDyww9MbSukeR4pGmxQR+Ube6LpJBA6c8TzHtey/GGOmG2NSjTGpBQUFXihDRETAO0FfVx+K/Z8F1s601iZba5Pj4+O9UIaIiIB3gj4H6HTG8yQg1wvbERGRevBG0K8HehljuhljmgO3AMu8sB0REakHj5+MtdZWGWN+AHwIhACvWmvTPL0dERGpH6/cGWutfQ94zxvvLSIiDRPwd8aKiMg3U9CLiDicsfZ/rnz0fRHGFABZDXhJW+CIl8ppyoJxv4NxnyE49zsY9xnc2+8u1tpzXp/eJIK+oYwxqdbaZH/X4WvBuN/BuM8QnPsdjPsMvtlvdd2IiDicgl5ExOECNehn+rsAPwnG/Q7GfYbg3O9g3GfwwX4HZB+9iIjUX6C26EVEpJ4CLuh9NXuVPxljOhljVhpjdhpj0owxD7iWtzHGfGSMSXd9b+3vWj3NGBNijNlkjPmX63k3Y8xa1z6/6Ro/yVGMMbHGmLeNMbtcx/zCIDnWD7o+39uNMQuMMeFOO97GmFeNMfnGmO1nLKvz2JpaL7iybasxZpin6giooPfl7FV+VgX8xFrbD0gB7nPt5wxgubW2F7Dc9dxpHgB2nvH8aeBZ1z4XAVP9UpV3PQ98YK3tCwymdv8dfayNMYnAD4Fka+0AasfFugXnHe85wNVnqraCAAACjklEQVRfW3a2Y3sN0Mv1NR142VNFBFTQc8bsVdbaCuD07FWOYq09ZK3d6HpcSu1//ERq93Wua7W5wHX+qdA7jDFJwFjg767nBhgNvO1axYn7HA2MAmYBWGsrrLXFOPxYu4QCEcaYUCASOITDjre19jOg8GuLz3ZsxwOv2VprgFhjTIIn6gi0oK/X7FVOYozpCgwF1gLtrbWHoPaXAdDOf5V5xXPAz4Aa1/M4oNhaW+V67sTj3R0oAGa7uqz+boyJwuHH2lp7EPgjkE1twB8DNuD84w1nP7Zey7dAC/p6zV7lFMaYlsA/gB9Za0v8XY83GWPGAfnW2g1nLq5jVacd71BgGPCytXYocAKHddPUxdUvPR7oBnQEoqjtuvg6px3vb+K1z3ugBX3QzF5ljAmjNuTnWWsXuxYfPv2nnOt7vr/q84KLgGuNMZnUdsmNpraFH+v60x6cebxzgBxr7VrX87epDX4nH2uAK4D91toCa20lsBgYifOPN5z92Hot3wIt6INi9ipX3/QsYKe19pkzfrQMuNP1+E5gqa9r8xZr7cPW2iRrbVdqj+sKa+3twErgRtdqjtpnAGttHnDAGNPHtehyYAcOPtYu2UCKMSbS9Xk/vd+OPt4uZzu2y4BJrqtvUoBjp7t43GatDagvYAywB9gL/MLf9XhpHy+m9k+2rcBm19cYavuslwPpru9t/F2rl/b/UuBfrsfdgXVABvAW0MLf9Xlhf4cAqa7j/Q7QOhiONfBrYBewHXgdaOG04w0soPYcRCW1LfapZzu21HbdvOTKtm3UXpHkkTp0Z6yIiMMFWteNiIg0kIJeRMThFPQiIg6noBcRcTgFvYiIwynoRUQcTkEvIuJwCnoREYf7f0m4AGO1r5mMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "data_all = data + [yhat]\n",
    "index = range(1,len(data_all)+1)\n",
    "\n",
    "plt.plot(index,data_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] fit and predict is very complex functions.\n",
    "- [ ] likelyhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average(MA)\n",
    "\\begin{equation*}\n",
    "y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\theta_q e_{t-q}\n",
    "\\end{equation*}\n",
    "- et: white noise\n",
    "- is a multiple regression with past errors as predictors\n",
    "- uses past forecast errors in a regression-like model\n",
    "- Don’t confuse this with moving average smoothing\n",
    "- The method is suitable for univariate time series without trend and seasonal components.\n",
    "- the average method assumes that all observations are of equal importance, and gives them equal weights when generating forecasts.\n",
    "    - [ ] How to decompose the trend and seasonal components?\n",
    "\\begin{equation*}\n",
    "\\hat{y_t} = \\frac{1}{m} \\sum y_{t+j}\n",
    "\\\\\n",
    "m = 2k+1,j= [-k,k]\n",
    "\\end{equation*}\n",
    "- It is possible to write any stationary AR( ) model as an MA( ) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74.71912804]\n"
     ]
    }
   ],
   "source": [
    "# MA example\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from random import random\n",
    "\n",
    "# contrived dataset\n",
    "data = [x+ random() for x in range(1,100)]\n",
    "\n",
    "# fit model\n",
    "model = ARMA(data,order=(0,1))\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data),len(data))\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25e89332fd0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "data_all = data + [yhat]\n",
    "index = range(1,len(data_all)+1)\n",
    "\n",
    "plt.plot(index,data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ARMA\n",
    "- with both MA&AR terms, no differencing\n",
    "\\begin{equation*}\n",
    "y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\theta_q e_{t-q}\n",
    "\\end{equation*}\n",
    "- Conditions on coefficients ensure stationarity.\n",
    "- Conditions on coefficients ensure invertibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57169792]\n"
     ]
    }
   ],
   "source": [
    "# ARMA example\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data = [random() for x in range(1, 100)]\n",
    "# fit model\n",
    "model = ARMA(data, order=(2, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data))\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ARIMA (Autoregressive Integrated Moving Average)\n",
    "\n",
    "- auto.arima in R: automatically selects the best one\n",
    "\n",
    "\\begin{equation*}\n",
    "y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + \\dots + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\theta_q e_{t-q}\n",
    "\\end{equation*}\n",
    "\n",
    "- y'_t : differenced series\n",
    "    - for d = 1, y'_t = y_t - y_{t-1}\n",
    "- AR: p = order of the autoregressive part\n",
    "- I: d = degree of first differencing involved\n",
    "    - for \n",
    "- MA: q = order of the moving average part.\n",
    "- The method is suitable for univariate time series with trend and without seasonal components.\n",
    "- Models written in terms of lagged variables (which ARIMA is) work only on equally-spaced time periods.\n",
    "- generative model   https://en.wikipedia.org/wiki/Generative_model\n",
    "- Backshift notation\n",
    "     - B operating on y_t, \n",
    "\\begin{equation*}\n",
    "By_t = y_{t-1}\n",
    "\\\\\n",
    "y'_t = y_t - y_{t-1} = y_t -  By_t = (1-B)y_t\n",
    "\\\\\n",
    "y\"_t = y_t - 2y_{t-1}+y_{t-2} = (1-B)^2y_t\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.55986804]\n"
     ]
    }
   ],
   "source": [
    "# ARIMA example\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from random import random\n",
    "# contrived dataset\n",
    "data = [x + random() for x in range(1, 100)]\n",
    "# fit model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "yhat = model_fit.predict(len(data), len(data), typ='levels')\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First order stationarity:\n",
    "    - A time series is first order stationary iff It is stationary in the mean (i.e. has constant mean).\n",
    "- ACF: Auto-Correlation Functions\n",
    "- PACF: Power  Auto-Correlation Functions\n",
    "- use ACF and PACF plot to determine appropriate values for p and q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA  Seasonal Autoregressive Integrated Moving-Average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Model : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Difference Equation\n",
    "- Ex1. Rabbit Reproduction\n",
    "    - Each pair of rabbits can reproduce from two months old\n",
    "    - Each reproduction produces only one pair of rabbits\n",
    "    - All rabbits survive\n",
    "        - season3 is season reproduce 1 pair and plut the parents pair.\n",
    "- s1. write the difference equation set\n",
    "    - system function\n",
    "    - the population relationship between this season and last season\n",
    "\\begin{equation*}\n",
    "M_{n+2} = M_{n+1} + M_n\n",
    "\\\\\n",
    "M_{n+1} = M_{n+1} + 0\n",
    "\\end{equation*}\n",
    "- s1. according the equation set, write the system Matric A:\n",
    "\\begin{equation*}\n",
    "A = \\binom{1,1}{1,0}\n",
    "\\\\\n",
    "\\mu_1 = A\\mu_0\n",
    "\\\\\n",
    "\\mu_n = \\binom{M_{n+1}}{M_{n}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- s3. write the characteristic equation:\n",
    "\\begin{equation*}\n",
    "\\binom{1-\\lambda,1}{1,-\\lambda} = 0\n",
    "\\end{equation*}\n",
    "- s4. calculate the eigenvalue \\lambda\n",
    "\\begin{equation*}\n",
    "\\lambda_{1,2} = \\frac{1\\pm \\sqrt{5}}{2}\n",
    "\\end{equation*}\n",
    "- s5. write out the eigendecomposition of A\n",
    "\\begin{equation*}\n",
    "A = \\frac{1}{\\lambda_1 - \\lambda_2} \\binom{\\lambda_1,\\lambda_2}{1,1} \\binom{\\lambda_1,0}{0,\\lambda_2} \\binom{1,-\\lambda_2}{-1,\\lambda_1} = \\binom{\\lambda_1 + \\lambda_2, -\\lambda_1 \\lambda_2}{1,0}\n",
    "\\end{equation*}\n",
    "- s6. write out the n generation A^n\n",
    "\\begin{equation*}\n",
    "A = SAS^{-1}\n",
    "\\\\\n",
    "A^n = SA^nS^{-1} = \\frac{1}{\\lambda_1 - \\lambda_2} \\binom{\\lambda_1,\\lambda_2}{1,1} \\binom{\\lambda_1^n,0}{0,\\lambda_2^n} \\binom{1,-\\lambda_2}{-1,\\lambda_1} = \\binom{\\lambda_1 + \\lambda_2, -\\lambda_1 \\lambda_2}{1,0}\n",
    "\\end{equation*}\n",
    "- put A^n in the model equation\n",
    "\\begin{equation*}\n",
    "\\mu_n = A^n \\mu_0 = SA^nS^{-1} \\mu_0\n",
    "\\\\\n",
    "\\mu_n = \\binom{u_1}{u_2} = c_1 \\lambda_1^n v_1 + c_2 \\lambda_2^2 v_2\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### :The Leslie Matrix\n",
    "- \n",
    "- normally test its stability \\lambda > 1 : increase\n",
    "\\begin{equation*}\n",
    "P(n) \\approx c_1\\lambda_1^n v_1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Growth\n",
    "- Discrete Form\n",
    "\\begin{equation*}\n",
    "M_{n+1} = M_n + kM_n (1-\\frac{M_n}{K} ) = M_n + (k - \\frac{M_n}{K}k)M_n\n",
    "\\end{equation*}\n",
    "- 1. Normal\n",
    "\\begin{equation*}\n",
    "\\frac{1}{N} \\frac{dN}{dt} = r(1-\\frac{N}{K})\n",
    "\\\\\n",
    "Solution: N(t) = \\frac{K}{(\\frac{K}{N_0}-1)e^{-rt}+1}\n",
    "\\end{equation*}\n",
    "- 2. Consider harvesting and culling term\n",
    "    - State is stable if a solution N(t) starting near N = c stays near it.\n",
    "    - that best culling time where y˙ is maximal.\n",
    "\\begin{equation*}\n",
    "\\frac{dy}{dt} = ry(1-y) -h\n",
    "\\\\\n",
    "y = N/K,h=H/K\n",
    "\\end{equation*}\n",
    "\n",
    "#### r-k Selection\n",
    "- r characteristics:\n",
    "    - short generation time,\n",
    "    - high fertility & ability to disperse offspring widely,\n",
    "    - early maturity onset with small body size.\n",
    "    - mosquitos,rats\n",
    "- K-selection\n",
    "    - long life expectancy,\n",
    "    - mate choice,\n",
    "    - fewer offspring but need extensive parental care to maturity\n",
    "    - large body size.\n",
    "    - elephants,humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Based Model \n",
    "\n",
    "#### Linear Regression\n",
    "```\n",
    "1. The relationship between X and Y is linear\n",
    "2. Y is distributed normally at each value of X\n",
    "3. The variance of Y at every value of X is the same (homogeneity of variances)\n",
    "4. The observations are independent\n",
    "```\n",
    "\n",
    "\n",
    "- s1. a linear model\n",
    "\\begin{equation*}\n",
    "M_w(d) = w[0] + w[1] * d[1]\n",
    "\\end{equation*}\n",
    "\n",
    "- s2. an error function\n",
    "    - this function involves the weight of the model, so optimal this function can optimal the model\n",
    "\\begin{equation*}\n",
    "L_2 (M_w, D) = 1/2 \\sum (t_i - M_w(d_i))^2\n",
    "\\\\\n",
    "M_w(d_i) = w \\cdot d\n",
    "\\end{equation*}\n",
    "\n",
    "- ith instance/entries, and jth features.\n",
    "\n",
    "- s3. optimise the error function\n",
    "    - error surface is convex and have a gloabl minimum\n",
    "\n",
    "#### Gradient Decent    \n",
    "- S1. selecting a random point within the weight space\n",
    "    - based on empirical evidence, choosing random initial weights uniformly from the range [−0.2, 0.2] tends to work well.\n",
    "- S2. calculate a prediction based on the initial weitht\n",
    "    - need a parameterized model such as LR\n",
    "- S3. calculate the error between target and prediction\n",
    "    - need an error function such as LSE\n",
    "- S4. calculate eerorDelta\n",
    "\\begin{equation*}\n",
    "errorDelta(D,w[j]) = - \\frac{\\partial L_2(M_w,D)}{\\partial w[j]} = \\sum ((t_i - M_w(d_i)) * d_i[j])\n",
    "\\end{equation*}\n",
    "- S5. update the w_j\n",
    "    - need decide learning rate\n",
    "    - Setting the Learning Rate Using Weight Decay\n",
    "\\begin{equation*}\n",
    "w[j] = w[j] + \\alpha * errorDelta(D,w[j])\n",
    "\\end{equation*} \n",
    "- S6. calculate new prediction based on the new weitht\n",
    "- S7. repeate S3-S6. untile get the convergence.\n",
    "- S8. at this stage, the new weight represent the model.\n",
    "- S9. Interprete the weight\n",
    "    - A better way to determine the importance of each descriptive feature in the model is to perform a statistical significance test\n",
    "\n",
    "- Categorical Features\n",
    "    - converts a single categorical descriptive feature into a number of continuous descriptive feature values that can encode the levels of the categorical feature.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- Handling Categorical Target Features\n",
    "- since the hard decision boundary is not continuous,so it is not differentiable : logistic function\n",
    "- datasets in which the instances with target features set to different levels overlap with each other in the feature space.\n",
    "\\begin{equation*}\n",
    "logistic(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}\n",
    "\\\\\n",
    "\\frac{dlogistic(x)}{dx} = logsitic(x) (1 - logistic(x))\n",
    "\\end{equation*}\n",
    "\n",
    "- logistc Regression Model\n",
    "\\begin{equation*}\n",
    "M_w(d) = logistic(w d) = \\frac{1}{1+e^{-wd}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- Cost function\n",
    "\\begin{equation*}\n",
    "Costf = - ylog(M_w(d)) - (1-y)long(1-M_w(d))\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- similar as s5 in linar regression\n",
    "\\begin{equation*}\n",
    "w[j] = w[j] + \\alpha * errorDelta(D,w[j]) = w[j] + \\alpha * \\sum ((t_i - M_w(d_i))*  M_w(d_i) * (1 - M_w(d_i)) * d_i[j])\n",
    "\\end{equation*}\n",
    "\n",
    "#### Normalizaiton\n",
    "- advantage\n",
    "    - The main advantages of normalizing descriptive feature values are that all weights become directly comparable with each other\n",
    "        - and the behavior of the gradient descent algorithm used to train the model becomes much less sensitive to the learning rate and the initial weights\n",
    "    - for logistic regression models we recommend that descriptive feature values always be normalized.\n",
    "- disadvantage\n",
    "    - more difficult for interprete the weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear Relationships\n",
    "- introduce basis functions that transform the raw inputs to the model into non-linear representations\n",
    "- lineaar regression for non-linear basis\n",
    "\\begin{equation*}\n",
    "M_w(d) = \\sum w[k] * \\phi_k(d)\n",
    "\\end{equation*}\n",
    "- a quadratic function basis example:\n",
    "\\begin{equation*}\n",
    "\\phi_1(d) = 1\n",
    "\\phi_2(d) = d\n",
    "\\phi_3(d) = d^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression\n",
    "- handles categorical target features with more than two levels\n",
    "- A good way to build multinomial logistic regression models is use a set of one-versus-all models.\n",
    "- main steps:\n",
    "- S1: For r target feature levels, we build r separate logistic regression models\n",
    "    - trained in parallel\n",
    "\\begin{equation*}\n",
    "M_{w_1}(d) = logistic(W_1 d)\n",
    "M_{w_2}(d) = logistic(w_2 d)\n",
    "\\end{equation*}\n",
    "- S2: Normalize for combine the different models\n",
    "    - revised,normalized prediction\n",
    "\\begin{equation*}\n",
    "M_{w_k}' = \\frac{M_{w_k} (d)}{\\sum{M_{w_k} (d)}\n",
    "\\end{equation*}\n",
    "- S3. use the revise,normalized prediction in the loss function\n",
    "    - the highest prediction, the class belong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "- http://bytesizebio.net/2014/02/05/support-vector-machines-explained-well/\n",
    "- Margin: This distance from the decision boundary to the nearest training instance is known as the margin\n",
    "- Training a support vector machine involves searching for the decision boundary, or separating hyperplane, that leads to the maximum margin as this will best separate the levels of the target feature.\n",
    "- Support Vectors: The instances in a training dataset that fall along the margin extents, and so define the margins, are known as the support vectors.\n",
    "- Support Vector Machine:\n",
    "    - When theoutput of this equation is greater than 1, we predict the positive target level for the query, and when the output is less than −1, we predict the negative target level.\n",
    "\\begin{equation*}\n",
    "M_{\\alpha,w_0}(q) = \\sum (t_i \\times \\alpha[i] \\times (d_i \\cdot q) + w_0)\n",
    "\\end{equation*}\n",
    "- constrained quadratic optimization problem\n",
    "- constrained part\n",
    "\\begin{equation*}\n",
    "t_i \\times (w_0 + w \\cdot d) \\ge 1\n",
    "\\end{equation*}\n",
    "- optimization criterion\n",
    "\\begin{equation*}\n",
    "dist(d) = \\frac{w_0 + w \\cdot d}{\\|w \\|}\n",
    "\\end{equation*}\n",
    "- The goal when training a support vector machine is to maximize subject\n",
    "\\begin{equation*}\n",
    "\\frac{2}{\\|w \\|}\n",
    "\\end{equation*}\n",
    "\n",
    "##### Non linear\n",
    "- basis functions can be used with support vector machines to handle training data that is not linearly separable.\n",
    "\\begin{equation*}\n",
    "t_i \\times (w_0 + w \\cdot \\phi(d)) \\ge 1\n",
    "\\end{equation*}\n",
    "- the new prediction model\n",
    "M_{\\alpha,w_0}(q) = \\sum (t_i \\times \\alpha[i] \\times (\\phi(d_i) \\cdot \\phi(q)) + w_0)\n",
    "- kernel trick\n",
    "    - A dot product of two high-dimensional vectors is a computationally expensive operation\n",
    "    - three kernel:\n",
    "        - linear/polynomial/Gaussian radial basis kernel\n",
    "    - It is best to start with a simple linear or low-degree polynomial kernel function and move to more complex kernel functions only if good performance cannot be achieved with this.\n",
    "\\begin{equation*}\n",
    "M_{\\alpha,kernel,w_0}(q) = \\sum (t_i \\times \\alpha[i] \\times kernel(d_i \\cdot q) + w_0)\n",
    "\\end{equation*}\n",
    "- Non-seperate problem: An extension of the standard support vector machine approach that allows a soft margin\n",
    "- not verey interpretable.\n",
    "\n",
    "##### SVR\n",
    "- https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff\n",
    "- for regression problem\n",
    "- include more points in the margin\n",
    "- minimal the distance between points and boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Model\n",
    "- extend ordinary regression to non-normal response distributions.\n",
    "- GLM’s allow us to analyze the linear relationship between predictor variables and the mean of the response variable when it is not reasonable to assume the data is distributed normally!\n",
    "- Response distribution must come from the Exponential Family of Distributions\n",
    "    - Includes Normal, Bernoulli, Binomial, Poisson, Gamma, etc.\n",
    "- Three Components:\n",
    "    - Random – \tIdentifies response Y and its probability distribution\n",
    "    - Systematic –  Explanatory variables in a linear predictor function (Xβ)\n",
    "    - Link function – Invertible function (g(.)) that links the mean of the response (E[Yi]=μi) to the systematic component.\n",
    "- Model Evaluation - -2 Log Likelihood\n",
    "    - Likelihood Ratio Test\n",
    "- Interpretation of Coefficient β – Odds Ratio\n",
    "- ROC curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Shapelets\n",
    "- Principle\n",
    "    - 1. Start with rough initial guesses for the shapelets,\n",
    "    - 2. Iteratively learn/optimize the shapelets by minimizing a classification loss function.\n",
    "\n",
    "- Notations in the paper:\n",
    "    - I: number of training instances\n",
    "    - Q: the length of the single time-series\n",
    "    - K: Number of Shapelets\n",
    "    - L: length of a Shapelets\n",
    "    - \\lambda_W : Regularization\n",
    "    - \\eta: Learning Rate\n",
    "    - maxIter: Number of iteration\n",
    "    - \\alpha controls the precision of the function and the soft minimum approaches the true minimum for\n",
    "\n",
    "- key Steps:\n",
    "- S1. A model function\n",
    "    - Logistic Regression model, the predictor is the distance bewteen canditate shapelets and every time series instances\n",
    "- S2. Loss function\n",
    "    - \n",
    "- S3. Objective function\n",
    "    - Same as logistic Regression with Regularization\n",
    "- S4. Gradient Descent\n",
    "    - updates the values of the shapelets and weights in the negative direction of the derivative with respect to the classification objective of each training instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tslearn Implementation\n",
    "- https://tslearn.readthedocs.io/en/latest/\n",
    "- http://fs.ismll.de/publicspace/LearningShapelets/\n",
    "\n",
    "-  time series is nothing more than a two-dimensional numpy array with its first dimension corresponding to the time axis and the second one being the feature dimensionality (1 by default).\n",
    "- Then, if we want to manipulate sets of time series, we can cast them to three-dimensional arrays, using to_time_series_dataset. If time series from the set are not equal-sized, NaN values are appended to the shorter ones and the shape of the resulting array is (n_ts, max_sz, d) where max_sz is the maximum of sizes for time series in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### S1. Import and split dataset\n",
    "```\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "\n",
    "datasets = UCR_UEA_datasets()\n",
    "dataset_list = datasets.list_datasets()\n",
    "\n",
    "ecg200 = datasets.load_dataset('ECG200')\n",
    "X_train, y_train, X_test, y_test = ecg200\n",
    "```\n",
    "\n",
    "##### S2. Transform Dataset\n",
    "```\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
    "X_test = TimeSeriesScalerMinMax().fit_transform(X_test)\n",
    "```\n",
    "\n",
    "##### S5. Initial the shapelet size\n",
    "- a parameter for shapelet model(next step)\n",
    "\n",
    "- Start with rough initial guesses for the shapelets,\n",
    "\n",
    "```\n",
    "from tslearn.shapelets import grabocka_params_to_shapelet_size_dict\n",
    "\n",
    "shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=X_train.shape[0],\n",
    "                                                       ts_sz=X_train.shape[1],\n",
    "                                                       n_classes=len(set(y_train)),\n",
    "                                                       l=0.1,\n",
    "                                                       r=2\n",
    "                                                       \n",
    "\n",
    "base_size = int(l * ts_sz)\n",
    "d = {}\n",
    "for sz_idx in range(r):\n",
    "    shp_sz = base_size * (sz_idx + 1)\n",
    "    //ts_sz - shp_sz + 1 is the sliding window segments number\n",
    "    // why need numpy.log10 ? For scale\n",
    "    n_shapelets = int(numpy.log10(n_ts * (ts_sz - shp_sz + 1) * (n_classes - 1)))\n",
    "    d[shp_sz] = n_shapelets\n",
    "return d\n",
    "                                                     \n",
    "```\n",
    "- n_ts (int) – Number of time series in the dataset : the I in paper\n",
    "- ts_sz (int) – Length of time series in the dataset: the Q in paper\n",
    "- n_classes (int) – Number of classes in the dataset\n",
    "- l (float) – Fraction of the length of time series to be used for base shapelet length\n",
    "    - as the K int paper 6.1.2 part\n",
    "- r (int) – Number of different shapelet lengths to use / a scale factor\n",
    "- return: dict, Dictionnary giving, for each shapelet length, the number of such shapelets to be generated\n",
    "- d[shp_sz] = n_shapelets\n",
    "- shp_sz: lenth of shapelets \n",
    "- n_shapelets: repectively number of shapelets. K in paper\n",
    "    - the orginal algorithm have a parameter K, much larger than gained from this algorithm\n",
    "\n",
    "\n",
    "##### S6: initial shapelet model\n",
    "- Iteratively learn/optimize the shapelets by minimizing a classification loss function\n",
    "- a novel classification model that is differentiable with respect to shapelets. Therefore, shapelets can be updated in a stochastic gradient descent optimization fashion, by taking steps towards the minimum of the classification loss function (i.e. towards maximal prediction accuracy).\n",
    "\n",
    "- for each shapests set combination, \n",
    "\n",
    "```\n",
    "shp_clf = ShapeletModel(n_shapelets_per_size=shapelet_sizes,\n",
    "                        optimizer=Adagrad(lr=.1),\n",
    "                        weight_regularizer=.01,\n",
    "                        max_iter=20000,\n",
    "                        verbose_level=0)\n",
    "```\n",
    "- n_shapelets_per_size: dict\n",
    "    - Dictionary giving, for each shapelet size (key),\n",
    "    - the number of such shapelets to be trained (value)\n",
    "- max_iter: int (default: 1000)\n",
    "    - Number of training epochs.\n",
    "- batch_size: int (default:256)\n",
    "    - Batch size to be used.\n",
    "- verbose_level: {0, 1, 2} (default: 2)\n",
    "    - `keras` verbose level.\n",
    "- optimizer: str or keras.optimizers.Optimizer (default: \"sgd\")\n",
    "    - https://keras.io/optimizers/\n",
    "    - `keras` optimizer to use for training.\n",
    "- weight_regularizer: float or None (default: None)\n",
    "    - Strength of the L2 regularizer to use for training the classification (softmax) layer. If None, no regularization is performed.\n",
    "- random_state : int or None, optional (default: None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.\n",
    " \n",
    "##### S7.  fit the model\n",
    "\n",
    "\n",
    "```\n",
    "shp_clf.fit(X_train, y_train)\n",
    "```\n",
    "- X : array-like of shape=(n_ts, sz, d)\n",
    "    - Time series dataset.\n",
    "- y : array-like of shape=(n_ts, )\n",
    "    - Time series labels.\n",
    "\n",
    "##### S8. predict\n",
    "```\n",
    "predicted_locations = shp_clf.locate(X_test)\n",
    "predicted_labels = shp_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "##### S9. visualize results\n",
    "```\n",
    "# plot time series instant\n",
    "plt.plot(X_test[test_ts_id].ravel())\n",
    "\n",
    "# plot shapelets\n",
    "for idx_shp, shp in enumerate(shp_clf1.shapelets_):\n",
    "    \n",
    "    # the Predictied_location is a matrics of shape[ts_n, shp_n]\n",
    "    # the i shapelet in jth timeseries.\n",
    "    t0 = predicted_locations[test_ts_id, idx_shp]\n",
    "    plt.plot(np.arange(t0, t0 + len(shp)), shp, linewidth=2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Shapelets\n",
    "- S1. Generating SAX Words\n",
    "\n",
    "```\n",
    "- defined in paper \"set the cardinality to 4, and word length to 16.\"\n",
    "\n",
    "        alphabet_size=4, sax_length=16\n",
    "\n",
    "\n",
    "- We specify the minimum and maximum length of the shapelet candidates that can be generated from this dataset as\n",
    "MINLEN and MAXLEN, respectively. For MAXLEN, we always set the longest possible length to the length of\n",
    "the shortest time series in the dataset. For MINLEN, we hardcoded the shortest possible\n",
    "length to three since three is the minimum meaningful length\n",
    "\n",
    "\n",
    "        if min_len is None:\n",
    "            min_len = sax_length\n",
    "        if max_len is None:\n",
    "            max_len = timeseries.shape[1]\n",
    "\n",
    "```\n",
    "- S2: Random Masking \n",
    "    - random iteration mask the SAX words to generate the collision table\n",
    "    - 'However, according to our experiments in Section 5,our algorithm is not sensitive to them; thus, we simply fix r = 10 and k = 10.'\n",
    "\n",
    "```\n",
    "    - self.iteration = 10 (r=10)\n",
    "```\n",
    "- S3. Generate candidate shapelets\n",
    "    - According the Distinguish Power from collision table to generate top_candidate\n",
    "    \n",
    "```\n",
    "nr_candidates=100\n",
    "```\n",
    "\n",
    "- S4: Calculate candidate information gain for best shapelets.\n",
    "\n",
    "```\n",
    "- the best shapelets number \n",
    "nr_shapelets=1\n",
    "```\n",
    "\n",
    "- S5: Combine\n",
    "\n",
    "```\n",
    "sax = SAXExtractor(alphabet_size=4, sax_length=16, nr_candidates=50,\n",
    "                   iterations=10, mask_size=3)\n",
    "shapelets_info = sax.extract(X_train, y_train, min_len=16, max_len = None, nr_shapelets=1)\n",
    "\n",
    "\n",
    "# Six items involeve in each elements of shaplets_info:\n",
    "# shape_info[0]: shapelets candidate\n",
    "# shape_info[1]: max information Gain\n",
    "# shape_info[2]: max separation gap\n",
    "# shape_info[3]: ts_idx which the selected shapelets come from\n",
    "# shape_info[4]: sax_idx, the idx of selected shapelets in certain ts\n",
    "# shape_info[5]: l: the lenth of the shapelets.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining Association Rules\n",
    "- assume all data are categorical\n",
    "- not good for numeric data\n",
    "- initially used for Market Basket Analysis \n",
    "\n",
    "#### Notations\n",
    "- I = {i_1,I_2,...,I_m} a set of items.\n",
    "- t: transaction: a set of items, and t ∈I.\n",
    "    - may have TID(transaction ID)\n",
    "- T: Transaction database: a set of transactions. \n",
    "    - T = {t_1,t_2,..,t_n}\n",
    "\n",
    "#### Association Rules:\n",
    "- A transaction t contains X(itemset/a pattern/ a set of items ) in I.\n",
    "- An association rule is an implication of the form:\n",
    "    - X-> Y, where X, Y ∈ I, and X ∩ Y = ∅\n",
    "- A k-itemset is an itemset with k items.\n",
    "\n",
    "##### Measure the strenght of Rules.\n",
    "- Support : transactions contain X ∪ Y\n",
    "    - sup = Pr(EX ∩ EY)\n",
    "- Confidence: transactions that contain X also contian Y.\n",
    "    - conf(X->Y) = Pr(EY|EX) = Pr(EX ∩ EY)/Pr(EX)\n",
    "- Lift: if the observed support were statistically independent\n",
    "    - sup/ Pr(EX) * Pr(EY)\n",
    "- Conviction: ration to that if the events were statidstically indepnedent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "- interpretable: each PC \n",
    "\n",
    "> https://skymind.ai/wiki/eigenvector\n",
    "\n",
    "- eigenvector : the direction of the wind\n",
    "    - the one that changes length but not direction by a matrix\n",
    "    - a n* n matrix have n eigenvectors.\n",
    "    - a 2x2 matrix have two eigenvectors[[0,1],[1,0]], it is Cartesian coordinate system(rectangular coordinate system)\n",
    "        - eigenvectors is Orthogonal \n",
    "\n",
    "- PCA concept\n",
    "    - 12 = 2 × 2 × 3: the decomposition give more information to the object(12), \n",
    "    - preprocess data for their neural networks.\n",
    "    - PCA attempts to draw straight, explanatory lines through data, like linear regression.\n",
    "    - Each straight line represents a “principal component,”,a relationship between an independent and dependent variable.\n",
    "- PCA on Covariance Matrix\n",
    "    - n * m matrics M_{n * m}: (n time-series, m features)\n",
    "    - M * M^T.shape == n* n : it is  a square and symmetric matrix. \n",
    "    - symmetric matrix, the eigenvalues are always real and the corresponding eigenvectors are always orthogonal.\n",
    "    - [ ] matrix discribe the covariance matrix\n",
    "    - Finding the eigenvectors and eigenvalues of the covariance matrix is the equivalent of fitting those straight, principal-component lines to the variance of the data. \n",
    "    - ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance.\n",
    "\t\n",
    "    \n",
    " - relation to Entropy & Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-based Learning\n",
    "- Do not need normalization\n",
    "\n",
    "- Infomation Gain:\n",
    "    - S1: Compute the entropy of the original dataset with respect to the target feature. This gives us an measure of how much information is required in order to organize the dataset into pure sets.\n",
    "        - note, the target feature\n",
    "\\begin{equation*}\n",
    "H(t,D) = - \\sum (P(t=l)*log_2(P(t=l)))\n",
    "\\end{equation*}   \n",
    "    - S2: For each descriptive feature, create the sets that result by partitioning the instances in the dataset using their feature values, and then sum the entropy scores of each of these sets. This gives a measure of the information that remains required to organize the instances into pure sets after we have split them using the descriptive feature.\n",
    "           - Expected information\n",
    "\\begin{equation*}\n",
    "rem(d,D) = \\sum \\frac{D_{d = l}}{D} * H(t,D=l)\n",
    "\\end{equation*} \n",
    "    - S3: Subtract the remaining entropy value (computed in step 2) from the original entropy value (computed in step 1) to give the information gain.\n",
    "\\begin{equation*}\n",
    "IG(d,D) = H(t,D) - rem(d,D)\n",
    "\\end{equation*}    \n",
    "    - S4. Compare the IG for each features, the more the IG, the better the features.\n",
    "    - S5. If one subpartition is empty, then use the most frequent target of the partition.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3\n",
    "- Iterative Dichotomizer 3 (ID3) algorithm\n",
    "    - attempts to create the shallowest decision tree that is consistent with the data given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "- Boosting:\n",
    "    - Each new model added to an ensemble is biased to pay more attention to instances that previous models misclassified.\n",
    "- Bagging:\n",
    "    - random sample with replacement, same size as the origin dataset.\n",
    "    - one model is induced from each bootstrap sample\n",
    "        - because decision trees are very sensitive to changes in the dataset\n",
    "        - for decision tree, the random relacement sampling is extended to random select on the features, known as subspace sampling.\n",
    "- Random Forest:\n",
    "    - S1: random sample on the train set, get a subset.\n",
    "    - S2: Subspace sampling on the subset. i.e. random select the features.\n",
    "    - S3: Grow a full ID3 tree on the Subspace subset\n",
    "    - S4: Make predictions by the majority vote of all the models in ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity based learning\n",
    "- Jaccard coefficient (similarity measure for asymmetric binary variables)\n",
    "\\begin{equation*}\n",
    "sim(i,j) = \\frac{a}{a+b+c}\n",
    "\\end{equation*}\n",
    "\n",
    "- SMC: simple matching coefficient.\n",
    "    - The main difference is that the SMC has the term {\\displaystyle M_{00}} M_{00} in its numerator and denominator\n",
    "\n",
    "- Gower’s Similarity Coefficient \n",
    "    - http://www.clustan.talktalk.net/gower_similarity.html\n",
    "    - It is applied for mixed data types, namely, databases with continuous, ordinal or categorical variables  at the same time.\n",
    "    - If all variables are binary, then Gower’s similarity coefficient is equivalent to the Jaccard’s similarity coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "##### Classification\n",
    "\n",
    "- S1. Compute the distance D(X,Xi) to every training example xi\n",
    "- S2. select k closest instance x1,...,xk and their labels (y set).\n",
    "- S3. output the yhat which is the most frequent in the y set.\n",
    "\n",
    "##### Regression\n",
    "- S1. Compute the distance D(X,Xi) to every training example xi\n",
    "- S2. select k closest instance x1,...,xk and their labels (y set).\n",
    "- S3. output the yhat which is the mean of the  y set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means cluster\n",
<<<<<<< HEAD
    "- http://flothesof.github.io/k-means-numpy.html"
=======
    "\n",
    "- K-Means clustering method considers two assumptions regarding the clusters \n",
    "    - first that the clusters are spherical and \n",
    "    - second that the clusters are of similar size. Spherical assumption helps in separating the clusters when the algorithm works on the data and forms clusters."
>>>>>>> f4d6e7bcf9e48806ea47ae92a3238b7e0e317304
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayesian\n",
    "\\begin{equation*}\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{\\sum P(x|y_i)P(y_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "- An “outlier” has a low probability under every class\n",
    "- Independence Assumption (this is when it becomes naive)\n",
    "    - chain rule to independent for likelihood\n",
    "        - P(B,S|H)=P(B|H)P(S|H)\n",
    "- For each target level, calculate the prior probability\n",
    "\n",
    "```\n",
    "- S1. the distribution of each predictor under the condition of each target level\n",
    "- S2. likelihood component for each target level\n",
    "    - continus: pdf\n",
    "    - discrete: conditional probability, x occur under each target level.\n",
    "\\begin{equation*}\n",
    "P(x | y) = P(x_i|y)P(x_j | y)\n",
    "\\end{equation*}\n",
    "- S3: posterior probability\n",
    "- S4. the big the choice\n",
    "```\n",
    "- Problems\n",
    "    - Zero-frequency problem laplace smoothing: add a small positive number to all counts\n",
    "    - missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture models\n",
    "- https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/\n",
    "\n",
    "- the key of this model is the generalized gaussian model\n",
    "\n",
    "- A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "- Expectation-maximization is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.\n",
    "\n",
    "- MLE is a frequentist principlethat suggests that given a dataset, the “best” parameters to use are the ones that maximise the probability of the data∗MLE is a way to formally pose the problem\n",
    "- EM is an algorithm∗EM is a way to solvethe problem posed by MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Bayesian Gaussian Mixture\n",
    "\n",
    "- Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood.\n",
    "- the variational algorithm needs more hyper- parameters than expectation-maximization\n",
    "\n",
    "\n",
    "- weight_concentration_prior: concentration prior: High values of the concentration prior will allow a larger number of components to be active in the mixture. most important hyperparameter\n",
    "\n",
    "- The Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model Technique\n",
    "- the process of seeking the optimal parameter of the model.\n",
    "- k1: the constrian or the optimal aim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE (Maximum Likelihood Estimation)\n",
    "- MLE does not eliminate sampling noise, or give us the truth.\n",
    "\n",
    "\n",
    "- S1. Stipulate a likelihood function for the data \n",
    "    - joint probability density function\n",
    "- S2. Take the logs(ln) of the likelihood function\n",
    "    - same maximum point\n",
    "- S3. Take the derivative of the log-likelihood function with respect to the parameters.\n",
    "- S4. Set the derivative to zero and solve the parameters\n",
    "- S5. Calculate the second parital derivatives with respect to the parameter, if possible , use matrix form.\n",
    "    - to check if the first order is the maximum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Design\n",
    "- https://newonlinecourses.science.psu.edu/stat501/node/347/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "- sklearn\n",
    "    - 思考你想做什么，你的输入和输出就明白了是哪种工种\n",
    "    - estimator\n",
    "    - predictor\n",
    "    - transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "- wrapper-based feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Model\n",
    "- http://modelselection.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian information criterion\n",
    "- http://modelselection.org/bic/\n",
    "- The lower the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- scoring: https://scikit-learn.org/stable/modules/model_evaluation.html#multimetric-scoring\n",
    "- https://stackoverflow.com/questions/38555650/try-multiple-estimator-in-one-grid-search\n",
    "\n",
    "- support multil param sets.\n",
    "- used for select hyperparameter rather than compare between different models.\n",
    "```\n",
    "S1. Initial Model with grid_para\n",
    "S2. initial GridSearchcv with model or pipe and the input data\n",
    "S3. After gridsearch,visualize the result.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross_validation\n",
    "- Cross_validataion is used for tuning the parameter and avoid overfitting.\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out Sampling\n",
    "- suit for large dataset\n",
    "- sometimes extended with evaluation set\n",
    "    - The validation set is used when data outside the training set is required in order to tune particular aspects of a model.\n",
    "    - avoid overfitting\n",
    "        - machine learning algorithms that iteratively build more and more complex models\n",
    "        - fitted to the nuances of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Fold Cross Validation\n",
    "- summing up the confusion matrix of each fold \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out Cross Validation\n",
    "- an extreme form of k-fold cross validation in which the number of folds is the same as the number of training instances\n",
    "- when the amount of data available is too small to allow big enough training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "- approximately fewer than 300 instances\n",
    "- Steps:\n",
    "    - S1: randomsly select m instances as test set\n",
    "    - S2： the ramainer is the train set\n",
    "    - s3: repeat k times, average the performance score\n",
    "        - k is set to values greater than or equal to 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-time Sampling\n",
    "- same as hold out, but sampeling a time span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Experiment\n",
    "\n",
    "- There is a summary of statistics test in the lecture slides: \"Multiple regression PPT\"\n",
    "\n",
    "- Precision and Recall\n",
    "- https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "- In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n",
    "- to determine which of the models that we have built for a particular task is most suitedto that task\n",
    "- to estimate how the model will perform when deployed\n",
    "- to convince the business for whom a model is being developed that the model will meet their needs\n",
    "- ANOVA\n",
    "- https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold out Test set\n",
    "- randomly sampling a portion of the data in the ABT we created in the Data Preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Statics\n",
    "\n",
    "- TP: \n",
    "    - 真正例是指模型将正类别样本正确地预测为正类别.\n",
    "    - predict the positive as the positive\n",
    "- FP:\n",
    "    - 模型将负类别样本错误地预测为正类别\n",
    "    - predict the negative target as the positive\n",
    "- TN:\n",
    "    - 真负例是指模型将负类别样本正确地预测为负类别\n",
    "    - predict the negative target as the negative\n",
    "- FN:\n",
    "    - predict the positive as the negative\n",
    "    - 模型将正类别样本错误地预测为负类别\n",
    "\n",
    "- Confusion matrix: work for binary target\n",
    "    - row index is Target\n",
    "    - column index is prediction\n",
    "\n",
    "\n",
    "- misclassification rate\n",
    "    - Number of incorrect predictions / total predictions\n",
    "    - (FP + FN) / (TP+FP + TN +FN)\n",
    "    - lower values indicate better performance    \n",
    "- classification accuracy\n",
    "    - (TP +TN) /(TP + FP + TN + FN)\n",
    "    - higher values indicate better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Exaperiment Design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Categorical Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix-based Performance Measures\n",
    "- true positive rate (TPR), true negative rate (TNR), false negative rate (FNR), and false positive rate (FPR)\n",
    "    - From the view of prediction\n",
    "- Precision:\n",
    "    - TP/(TP+FP): 100 positive instance(80 T, 20 F), 100(60T, 40F) negative instance, The prediction 80/80+40,\n",
    "    - Select a instance which predicted as positive, how confident we can trust it is a positive.\n",
    "    - From the view of prediction.\n",
    "- Recall:\n",
    "    - TP / (TP + FN) :  100 positive instance, 80 is True, 20 is False, recall is 80%\n",
    "    - equal to TPR\n",
    "    - how confident we can be that all the instances with the positive target level have been found by the model.\n",
    "    - From the view of target\n",
    "- F1:\n",
    "    - Precision* Recall /(Precision + Recall) \n",
    "    - https://en.wikipedia.org/wiki/F1_score\n",
    "- multiclass:\n",
    "    - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    ">>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    ">>> (tn, fp, fn, tp)\n",
    "(0, 2, 1, 1)\n",
    "\n",
    "# if average = 'binary', then the precision and recall is the maximum of the two class.\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(y_test,y_pred,average='binary')\n",
    "\n",
    "```\n",
    " \n",
    "-  Average class accuracy - arithmetic mean\n",
    "\\begin{equation*}\n",
    "acc_am = \\frac{1}{levels No. of target} \\sum recall_i \n",
    "\\end{equation*}\n",
    "\n",
    "- Average class accuracy - harmonic mean\n",
    "\\begin{equation*}\n",
    "acc_hm = \\frac{1}{\\frac{1}{levels No. of target} \\sum \\frac{1}{Recall_i}}\n",
    "\\end{equation*} \n",
    "\n",
    "- when calculating average class accuracy, the harmonic mean should be used rather than the arithmetic mean.\n",
    "- Profit matrix\n",
    "    - The actual values in a profit matrix are determined through domain expertise.\n",
    "- Recall and Precison analysis\n",
    "    - high recall + high precision : the class is perfectly handled by the model\n",
    "    - low recall + high precision : the model can’t detect the class well but is highly trustable when it does\n",
    "    - high recall + low precision : the class is well detected but the model also include points of other classes in it\n",
    "    - low recall + low precision : the class is poorly handled by the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Prediction Scores\n",
    "- The basis of most of these approaches is measuring how well the distributions of scores produced by the model for different target levels are separated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curves\n",
    "- TPR vs TNR: show the trade-off between this two index.\n",
    "    - with increase of threshold, TRP increase and TNR decrease.\n",
    "- ROC index (area under curve (AUC)) [0.5, 1]\n",
    "    - rule of thumb is that a value above 0.7 indicates astrong model, while a value below 0.6 indicates a weak model.\n",
    "    \n",
    "    - ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n",
    "    - Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "    - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
    "    \n",
    "- Gini coefficient: linear rescaling of the ROC index, [0,1]\n",
    "    - gini coefficint = (2* ROC index) \n",
    "- Kolmogorov-Smirnov Statistic\n",
    "    - CP(A,ps)  = (number of A (the target) with score) < ps / (number of A (the target))\n",
    "    - KS  = max CP_high - CP_low\n",
    "        - maximum distance\n",
    "- Measuring Gain and Lift\n",
    "    - customer relationship management\n",
    "    - the sutle difference , for focus on postive rather than distinguish\n",
    "    - The gain and lift measures attempt to measure to what extent a set of predictions made by a model meet this assumption\n",
    "        - assumption: the majority of the positive instances to be toward the top of this ranking.\n",
    "        - decending order the table\n",
    "    - Gain is a measure of how many of the positive instances in the overall test set are found in a particular decile\n",
    "        - the number of positive in each decile(10%) / the number of positive in full test set\n",
    "        - the closer the cumulative gain line is to the top left hand corner of the chart, the better the model is performing\n",
    "    - lift :\n",
    "        - the percentage of positive in decile / percentage of positive of the whole set.\n",
    "        - Lift can take values in the range [0, ∞], and higher values indicate that a model is performing well at a particular decile.\n",
    "        - cumlative percentage\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Multinomial Targets\n",
    "- recall / precision\n",
    "- based on prediction scores not easy,such as ROC etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures: Continuous Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Measures of Error\n",
    "- SSE: sum of squared error\n",
    "- MSE: Mean of squared error\n",
    "- RMSE:\n",
    "    - Root mean squared error values are in the same units as the target value\n",
    "- MAE: does not include a squared term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Independent Measures of Error\n",
    "- R^2 coefficient\n",
    "- [0,1), larger value, the better model\n",
    "\\begin{equation*}\n",
    "R^2 = 1- \\frac{sum of squared error}{total sum of squares}\n",
    "\\\\\n",
    "total sum of squares = (1/2) \\sum (t_i - \\bar{t})^2\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models after Deployment\n",
    "- concept drift\n",
    "    - almost all the predictive models that we build will at some point go stale\n",
    "    - on-going model validation scheme\n",
    "### Monitoring Changes in Performance Measures\n",
    "- problem: entirely domain dependent\n",
    "- correct target feature value\n",
    "### Monitoring Model Output Distribution Changes  \n",
    "- stability index\n",
    "- 0.1 : 0.25\n",
    "\\begin{equation*}\n",
    "stability index = \\sum ((\\frac{|A_{t=l}|}{A}- \\frac{B_{t=l}}{B}) * ln(\\frac{|A_{t=l}|}{A} / \\frac{B_{t=l}}{B}))\n",
    "\\end{equation*} \n",
    "\n",
    "### Monitoring Descriptive Feature Distribution Changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For time-series\n",
    "- The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when 􀁿tting the model.\n",
    "\n",
    "\n",
    "### Stationality\n",
    "- A time series is first order stationary iff It is stationary in the mean (i.e. has constant mean).\n",
    "- A time series is second order stationary iff It is stationary in the mean & variance (constant mean, variance, covariance).\n",
    "    - the timing of these cycles is not predictable. Hence the series is stationary\n",
    "\n",
    "### Residuals: what is left over after fitting a model\n",
    "    - A good forecasting method:\n",
    "        - The residuals are uncorrelated.\n",
    "            - if not: then there is information left in the residuals\n",
    "        - The residuals have zero mean.\n",
    "            - If the residuals have a mean other than zero, then the forecasts are biased.\n",
    "        - The residuals have constant variance\n",
    "        - The residuals are normally distributed\n",
    "\\begin{equation*}\n",
    "e_t = y_t - \\hat{y_t}\n",
    "\\end{equation*}\n",
    "\n",
    "### Traing and Test set\n",
    "- The test set should ideally be at least as large as the maximum forecast horizon required.\n",
    "\n",
    "### Forecast errors\n",
    "- not the mistake it means the unpredictable part of an observation\n",
    "-  VS Residuals:\n",
    "    - residuals are calculated on the training set while forecast errors are calculated on the test set\n",
    "    - Second, residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts.\n",
    "\n",
    "### Scale-dependent errors\n",
    "#### MAE (Mean absolute error)\n",
    "\\begin{equation*}\n",
    "MAE = mean(|e_t|)\n",
    "\\end{equation*}\n",
    "\n",
    "#### RMSE (Root mean squared error)\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{mean(e_t^2)}\n",
    "\\end{equation*}\n",
    "- A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the\n",
    "mean.\n",
    "\n",
    "### Percentage errors\n",
    "- unit-free\n",
    "#### MAPE (Mean absolute percentage error)\n",
    "- disadvantage:\n",
    "    - if y is small or to zero. the p_t is very large\n",
    "    - put a heavier penalty on negative errors than on positive errors\n",
    "\\begin{equation*}\n",
    "MAPE = mean(|p_t|)\n",
    "\\\\\n",
    "p_t = 100e_t/y_t\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Scaled errors\n",
    "#### MASE(Mean absolute scaled error)\n",
    "\n",
    "\n",
    "#### RMSLE root mean squared logarithmic error.\n",
    "    - often used in kaggle regression problem\n",
    "```\n",
    "def rmsle(real, predicted):\n",
    "    sum=0.0\n",
    "    for x in range(len(predicted)):\n",
    "        if predicted[x]<0 or real[x]<0: #check for negative values\n",
    "            continue\n",
    "        p = np.log(predicted[x]+1)\n",
    "        r = np.log(real[x]+1)\n",
    "        sum = sum + (p - r)**2\n",
    "    return (sum/len(predicted))**0.5\n",
    "```\n",
    "\n",
    "### Cross validation\n",
    "- test is a single observation\n",
    "- forecast accuracy is computed by averaging over the test sets\n",
    "- A good way to choose the best forecasting model is to 􀁿nd the model with the smallest RMSE computed using time series cross-validation.\n",
    "- https://robjhyndman.com/hyndsight/tscv/\n",
    "\n",
    "- prophet\n",
    "- rolling origin\" forecast evaluation procedures / evaluation on a rolling forecasting origin”\n",
    "\n",
    "\n",
    "### Prediction intervals\n",
    "- a 95% prediction interval \n",
    "    - contains a range of values which should include the actual future value with probability 95%.\n",
    "    - The set of values that this random variable could take, along with their relative probabilities, is known as the “probability distribution”\n",
    "- More generally, a prediction interval can be written as\n",
    "\\begin{equation*}\n",
    "\\hat{y}_{T+h |T} \\\n",
    "\\end{equation*}\n",
    "    - where the standart devision is from the model we used for the forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test\n",
    "- S1.Clear the test statistics\n",
    "    - mean or variance\n",
    "- S2. Select the suitabel test method:\n",
    "    - Note: below test assumpt that the population is normal distribution.\n",
    "        - from the textbook table 8-1.\n",
    "    - Z-test: \\mu for variance is known\n",
    "\\begin{equation*}\n",
    "Z = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n",
    "\\end{equation*} \n",
    "    - T-test(t(n-1)): \\mu for variance is unkonwn\n",
    "\\begin{equation*}\n",
    "t = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\n",
    "\\end{equation*}    \n",
    "    - Z-test: the distance \\delta between two mean for var1 and var2 is known\n",
    "       - two subgroups have same mean\n",
    "\\begin{equation*}\n",
    "Z = \\frac{\\bar{X} - \\bar{Y} - \\delta }{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} }}\n",
    "\\end{equation*}    \n",
    "    - T-test(t(n1+n2-2)): the distance \\delta between two mean for var1 and var2 is unknown \n",
    "\\begin{equation*}\n",
    "t = \\frac{\\bar{X} - \\bar{Y} - \\delta }{S_w\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2} }}\n",
    "\\\\\n",
    "S_w = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1+n_2 - 2}\n",
    "\\end{equation*} \n",
    "    - K_square-test: var  = var0, mean0 is unknown.\n",
    "\\begin{equation*}\n",
    "\\chi^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\n",
    "\\end{equation*}\n",
    "    - F-test\n",
    "\\begin{equation*}\n",
    "F = \\frac{S_1^2}{S_2^2}\n",
    "\\end{equation*}    \n",
    "- S3: According the test method and significance level to calculate the k-value(same unit the quantile)\n",
    "- S4: According the sampel observation value to calculate the test statistic.\n",
    "- S5: Compare the test statistic value and k-value to determine accept or reject the hypothesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Fitting Test\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP\n",
    "- https://blog.csdn.net/u011508640/article/details/72815981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Model\n",
    "- Stable\n",
    "    - If all eigenvalues |\\lambda_i| < 1, system is stable & un ! 0 as n unlimit.\n",
    "    - Whenever all values satisfy |\\lambda_i| < = 1, system is neutrally stable & un is bounded as n unlimit.\n",
    "    - Whenever at least one value satisfies |\\lambda_i| > 1, system is unstable & un is unbounded as n unlimit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "- BLUE:\n",
    "    - best linear unbiased estimator\n",
    "    - the sample linear regression can get by ordinary least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpertation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Linear Regression\n",
    "- First, the signs of the weights indicate whether different descriptive features have a positive or a negative impact on the prediction.\n",
    "- the magnitudes of the weights show how much the value of the target feature changes for a unit change in the value of a particular descriptive feature.\n",
    "- A better way to determine the importance of each descriptive feature in the model is to perform a statistical significance test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "- https://www.zhihu.com/question/65171922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging\n",
    "- https://packaging.python.org/tutorials/packaging-projects/#packaging-your-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liscense\n",
    "\n",
    "- https://choosealicense.com/\n",
    "- https://www.cnblogs.com/Wayou/p/how_to_choose_a_license.html\n",
    "- GPL: 要作为工具分发软件的话，必须开源你的源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show\n",
    "- https://mybinder.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Mathematics\n",
    "- derivate is from limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra\n",
    "### 行列式\n",
    "- 行列式有几个重要的变换性质，可以用来计算行列式。\n",
    "    - 对角线法则，主对角线（左上到右下） - 副对角线\n",
    "    - 行或列的互换\n",
    "    - 某一行加上另一行的乘以某个数\n",
    "    - 某一行或列的元素与其对应的代数余子式之和。\n",
    "- 矩阵变换：\n",
    "    - 想象在解方程组，对系数的变换/消元都不影响最后的解。\n",
    "- characteristic equation:\n",
    "    - 一个矩阵与其特征向量相乘，只改变其长度，不改变方向（各元素的比例）\n",
    "\\begin{equation*}\n",
    "det(A - \\lambda I) = 0\n",
    "\\\\\n",
    "\\lambda^2 - p\\lambda + q = 0\n",
    "\\\\\n",
    "p = a_11 + a_22; q = a_11 a_22 - a_12 a_21\n",
    "\\\\\n",
    "\\lambda_{1,2} = \\frac{p}{2} \\pm \\frac{\\sqrt{p^2 - 4q}}{2}\n",
    "\\\\\n",
    "\\lambda_1 + \\lambda_2 = - \\frac{b}{a} = -\\frac{-p}{1} = p\n",
    "\\\\\n",
    "\\lambda_1 * \\lambda_2 = \\frac{c}{a} = \\frac{q}{1} = q\n",
    "\\end{equation*}\n",
    "\n",
    "- for second order matrix, a eigendecomposition:\n",
    "    - this is the most important key formula for problem.\n",
    "    - S has eigenvectors of A, v1, v2 on the columns\n",
    "\\begin{equation*}\n",
    "A = \\frac{1}{\\lambda_1 - \\lambda_2} \\binom{\\lambda_1,\\lambda_2}{1,1} \\binom{\\lambda_1,0}{0,\\lambda_2} \\binom{1,-\\lambda_2}{-1,\\lambda_1} = \\binom{\\lambda_1 + \\lambda_2, -\\lambda_1 \\lambda_2}{1,0}\n",
    "\\\\\n",
    "A^n = \\frac{1}{\\lambda_1 - \\lambda_2} \\binom{\\lambda_1,\\lambda_2}{1,1} \\binom{\\lambda_1^n,0}{0,\\lambda_2^n} \\binom{1,-\\lambda_2}{-1,\\lambda_1} = \\binom{\\lambda_1 + \\lambda_2, -\\lambda_1 \\lambda_2}{1,0}\n",
    "\\\\\n",
    "S * S^{-1} = \\dbinom{\\lambda_1 - \\lambda_2,0}{0,\\lambda_1 - \\lambda_2}\n",
    "\\\\\n",
    "S = (v1,v2)\n",
    "\\\\\n",
    "v1 = C \\dbinom{\\lambda_1}{1}, v2 = C \\dbinom{\\lambda_2}{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- a second order ODE system has a solution:\n",
    "\\begin{equation*}\n",
    "\\mathbf{x(t)} = \\dbinom{x(t)}{y(t)} = c_1 v_1 e^{\\lambda_1t} + c_2 v_2 e^{\\lambda_2t}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "- https://spssau.com/front/spssau/helps/visualization/scatter.html\n",
    "- https://www.itl.nist.gov/div898/handbook/index.htm\n",
    "- 统计学的核心思想是根据样本的指标来推断总体的表现：\n",
    "    - 比如估计总体的均值，方差，统计模型等等\n",
    "    - 评估同分布，不同总体是否一致，ANOVA检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation (arrangement) and combiantion\n",
    "\n",
    "- because permutation consider the order, so same number in different positon represent different set. so permutation is bigger than combination\n",
    "\n",
    "\\begin{equation*}\n",
    "A_n^r = \\frac{n!}{r!}\n",
    "\\end{equation*} \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "C_n^r = \\frac{n!}{(n-1)!r!}\n",
    "\\end{equation*} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code style\n",
    "- http://google.github.io/styleguide/pyguide.html"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离开rui多少天了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have left 164 days\n"
     ]
    }
   ],
   "source": [
    "start = pd.to_datetime('2019-12-29').date()\n",
    "today = pd.datetime.now().date()\n",
    "period = today - start\n",
    "print(\"Have left {} days\".format(period.days))"
   ]
  },
  {
>>>>>>> f4d6e7bcf9e48806ea47ae92a3238b7e0e317304
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "578px",
    "width": "547px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "400.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
