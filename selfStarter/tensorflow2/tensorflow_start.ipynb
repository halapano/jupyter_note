{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Log\n",
    "- 2020-5-23 11:35:28， packeet版本的Deep Learning with Tensorflow 2.0，基本上只讲了实施过程，对每一步的解释和说明都不太透彻，转到另外一本书 Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow， 这边书首先将cnn就还不错。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "- Book: Deep Learning with Tensorflow 2.0\n",
    "    - https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-2-and-Keras\n",
    "    \n",
    "- https://github.com/tensorflow/tensorflow\n",
    "- https://github.com/halapano/handson-ml2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter1 Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Keras:\n",
    "- Keras is a beautiful API for composing building blocks to create and train deep\n",
    "learning models. Keras can be integrated with multiple deep learning engines\n",
    "including Google TensorFlow, Microsoft CNTK, Amazon MxNet, and Theano.\n",
    "Starting with TensorFlow 2.0, Keras has been adopted as the standard high-level\n",
    "API, largely simplifying coding and making programming more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0 vs 1.0\n",
    "- TensorFlow 2.0 is to make TensorFlow easier to learn and to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "```\n",
    "In one sentence, machine learning models are a way to compute a function that maps\n",
    "some inputs to their corresponding outputs. The function is nothing more than a\n",
    "number of addition and multiplication operations. However, when combined with\n",
    "a non-linear activation and stacked in multiple layers, these functions can learn\n",
    "almost anything [8]. You also need a meaningful metric capturing what you want to\n",
    "optimize (this being the so-called loss function that we will cover later in the book),\n",
    "enough data to learn from, and sufficient computational power.\n",
    "\n",
    "\n",
    "key words: activation function/ objective function/ \n",
    "```\n",
    "\n",
    "- Activation function \n",
    "    - activation function is small classifier which map x to y.\n",
    "    - sigmoid\n",
    "    - tanh\n",
    "    - ReLU (REctified Linear Unit)\n",
    "- objective function: loss/cost function\n",
    "    - MSE\n",
    "    - binary_crossentropy\n",
    "    - categorical_crossentropy\n",
    "- first improvment:\n",
    "    - add hidden layer\n",
    "- second improvment:\n",
    "    - dropout randomly: The idea behind this improvement is that random dropout forces the network to learn redundant patterns that are useful for better generalization.\n",
    "- third improvment:\n",
    "    - change optimiser: SGD, RMSProp and Adam\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# code p15\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# Network and training parameters.\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "DROPOUT = 0.3 # Seccond Improvment\n",
    "\n",
    "# Loading MNIST dataset.\n",
    "# verify\n",
    "# You can verify that the split between train and test is 60,000, and 10,000 respectively.\n",
    "# Labels have one-hot representation.is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we --> reshape it to\n",
    "# 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representation of the labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(NB_CLASSES,\n",
    "#             input_shape=(RESHAPED,),\n",
    "#             name='dense_layer',\n",
    "#             activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model. \n",
    "# Improve first time. add two hidden layer\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "            input_shape=(RESHAPED,),\n",
    "            name='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "            name='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "            name='dense_layer_3', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#                 loss='categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])\n",
    "\n",
    "# third improvement: optimizer = \"RMSProp\"\n",
    "model.compile(optimizer=\"RMSProp\",\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 5s 108us/sample - loss: 0.3904 - accuracy: 0.8850 - val_loss: 0.1605 - val_accuracy: 0.9554\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.1662 - accuracy: 0.9510 - val_loss: 0.1269 - val_accuracy: 0.9638\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.1164 - accuracy: 0.9666 - val_loss: 0.1027 - val_accuracy: 0.9696\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0886 - accuracy: 0.9732 - val_loss: 0.0955 - val_accuracy: 0.9715\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.0882 - val_accuracy: 0.9747\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0609 - accuracy: 0.9819 - val_loss: 0.0904 - val_accuracy: 0.9772\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0515 - accuracy: 0.9842 - val_loss: 0.0977 - val_accuracy: 0.9744\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0433 - accuracy: 0.9866 - val_loss: 0.0972 - val_accuracy: 0.9743\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.0955 - val_accuracy: 0.9769\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0338 - accuracy: 0.9896 - val_loss: 0.1032 - val_accuracy: 0.9762\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.1015 - val_accuracy: 0.9777\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 3s 52us/sample - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.1141 - val_accuracy: 0.9748\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.1127 - val_accuracy: 0.9769\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0223 - accuracy: 0.9928 - val_loss: 0.1150 - val_accuracy: 0.9771\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.1181 - val_accuracy: 0.9779\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.1275 - val_accuracy: 0.9755\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0147 - accuracy: 0.9948 - val_loss: 0.1299 - val_accuracy: 0.9759\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.1334 - val_accuracy: 0.9769\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.1550 - val_accuracy: 0.9776\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.1386 - val_accuracy: 0.9784\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0119 - accuracy: 0.9960 - val_loss: 0.1619 - val_accuracy: 0.9765\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.1580 - val_accuracy: 0.9787\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.1641 - val_accuracy: 0.9768\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.1698 - val_accuracy: 0.9758\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.1600 - val_accuracy: 0.9775\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.1731 - val_accuracy: 0.9761\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0092 - accuracy: 0.9966 - val_loss: 0.1707 - val_accuracy: 0.9762\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.1861 - val_accuracy: 0.9772\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1832 - val_accuracy: 0.9778\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.1920 - val_accuracy: 0.9758\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.1980 - val_accuracy: 0.9776\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.2118 - val_accuracy: 0.9763\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.2195 - val_accuracy: 0.9753\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.2123 - val_accuracy: 0.9762\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.1959 - val_accuracy: 0.9780\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.2102 - val_accuracy: 0.9781\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 2s 52us/sample - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.2088 - val_accuracy: 0.9784\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.2207 - val_accuracy: 0.9775\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.2258 - val_accuracy: 0.9768\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.2215 - val_accuracy: 0.9765\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.2482 - val_accuracy: 0.9757\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2416 - val_accuracy: 0.9777\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.2359 - val_accuracy: 0.9774\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.2354 - val_accuracy: 0.9762\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.2275 - val_accuracy: 0.9774\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.2514 - val_accuracy: 0.9769\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.3243 - val_accuracy: 0.9728\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.2553 - val_accuracy: 0.9773\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.2571 - val_accuracy: 0.9785\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.2606 - val_accuracy: 0.9779\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.2529 - val_accuracy: 0.9767\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.2577 - val_accuracy: 0.9776\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.2680 - val_accuracy: 0.9758\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2670 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 2s 39us/sample - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.2901 - val_accuracy: 0.9758\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.2919 - val_accuracy: 0.9773\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.2876 - val_accuracy: 0.9756\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.3027 - val_accuracy: 0.9760\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.3106 - val_accuracy: 0.9768\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2864 - val_accuracy: 0.9778\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2869 - val_accuracy: 0.9776\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.3255 - val_accuracy: 0.9766\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.3173 - val_accuracy: 0.9777\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.3004 - val_accuracy: 0.9779\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.2854 - val_accuracy: 0.9773\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.3086 - val_accuracy: 0.9783\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.3075 - val_accuracy: 0.9778\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 2s 42us/sample - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.3020 - val_accuracy: 0.9778\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.3167 - val_accuracy: 0.9785\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.2876 - val_accuracy: 0.9788\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 2s 43us/sample - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.3304 - val_accuracy: 0.9769\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.3034 - val_accuracy: 0.9763\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.3175 - val_accuracy: 0.9791\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 3s 56us/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.3345 - val_accuracy: 0.9764\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 2s 52us/sample - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.3271 - val_accuracy: 0.9777\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 3s 53us/sample - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.3328 - val_accuracy: 0.9770\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.3197 - val_accuracy: 0.9777\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3593 - val_accuracy: 0.9768\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.3494 - val_accuracy: 0.9776\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.3652 - val_accuracy: 0.9782\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.3570 - val_accuracy: 0.9772\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.3509 - val_accuracy: 0.9768\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.3469 - val_accuracy: 0.9767\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.3646 - val_accuracy: 0.9768\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.3994 - val_accuracy: 0.9778\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.3901 - val_accuracy: 0.9757\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.3805 - val_accuracy: 0.9786\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.3592 - val_accuracy: 0.9779\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.3899 - val_accuracy: 0.9769\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 3s 53us/sample - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.4080 - val_accuracy: 0.9776\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.3961 - val_accuracy: 0.9781\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.3437 - val_accuracy: 0.9786\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.4104 - val_accuracy: 0.9768\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.3564 - val_accuracy: 0.9783\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.3720 - val_accuracy: 0.9757\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.3489 - val_accuracy: 0.9774\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 2s 52us/sample - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.3842 - val_accuracy: 0.9741\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 3s 56us/sample - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.3875 - val_accuracy: 0.9770\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.4305 - val_accuracy: 0.9754\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.3968 - val_accuracy: 0.9776\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.3712 - val_accuracy: 0.9772\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.3785 - val_accuracy: 0.9770\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.4030 - val_accuracy: 0.9757\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0080 - accuracy: 0.9988 - val_loss: 0.3912 - val_accuracy: 0.9787\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.3878 - val_accuracy: 0.9783\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.4095 - val_accuracy: 0.9762\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.4020 - val_accuracy: 0.9772\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0073 - accuracy: 0.9984 - val_loss: 0.4150 - val_accuracy: 0.9772\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.4143 - val_accuracy: 0.9780\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.3820 - val_accuracy: 0.9778\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0073 - accuracy: 0.9987 - val_loss: 0.4110 - val_accuracy: 0.9768\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.4133 - val_accuracy: 0.9772\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0081 - accuracy: 0.9986 - val_loss: 0.4134 - val_accuracy: 0.9764\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.4213 - val_accuracy: 0.9762\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.4260 - val_accuracy: 0.9762\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.4483 - val_accuracy: 0.9787\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.4228 - val_accuracy: 0.9766\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.4366 - val_accuracy: 0.9761\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.4391 - val_accuracy: 0.9757\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.4716 - val_accuracy: 0.9751\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.4573 - val_accuracy: 0.9774\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.4674 - val_accuracy: 0.9773\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0066 - accuracy: 0.9987 - val_loss: 0.4359 - val_accuracy: 0.9770\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.4339 - val_accuracy: 0.9768\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.4547 - val_accuracy: 0.9761\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0077 - accuracy: 0.9987 - val_loss: 0.4584 - val_accuracy: 0.9764\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0075 - accuracy: 0.9986 - val_loss: 0.4937 - val_accuracy: 0.9771\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.4528 - val_accuracy: 0.9777\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.4362 - val_accuracy: 0.9777\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.4126 - val_accuracy: 0.9784\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.4739 - val_accuracy: 0.9770\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0068 - accuracy: 0.9986 - val_loss: 0.4392 - val_accuracy: 0.9783\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0077 - accuracy: 0.9986 - val_loss: 0.4687 - val_accuracy: 0.9778\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.5446 - val_accuracy: 0.9763\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0069 - accuracy: 0.9986 - val_loss: 0.4565 - val_accuracy: 0.9783\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.4617 - val_accuracy: 0.9775\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0078 - accuracy: 0.9986 - val_loss: 0.4802 - val_accuracy: 0.9758\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.5365 - val_accuracy: 0.9758\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 2s 44us/sample - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.4608 - val_accuracy: 0.9778\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0095 - accuracy: 0.9988 - val_loss: 0.5057 - val_accuracy: 0.9771\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0100 - accuracy: 0.9987 - val_loss: 0.5056 - val_accuracy: 0.9775\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.4499 - val_accuracy: 0.9778\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.0109 - accuracy: 0.9981 - val_loss: 0.4408 - val_accuracy: 0.9763\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.4938 - val_accuracy: 0.9778\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.5162 - val_accuracy: 0.9767\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.4887 - val_accuracy: 0.9778\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.4991 - val_accuracy: 0.9765\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.5238 - val_accuracy: 0.9743\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.5385 - val_accuracy: 0.9759\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0105 - accuracy: 0.9984 - val_loss: 0.5140 - val_accuracy: 0.9753\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0104 - accuracy: 0.9983 - val_loss: 0.4462 - val_accuracy: 0.9784\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.5193 - val_accuracy: 0.9780\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0114 - accuracy: 0.9982 - val_loss: 0.5296 - val_accuracy: 0.9776\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0096 - accuracy: 0.9984 - val_loss: 0.4629 - val_accuracy: 0.9762\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.5215 - val_accuracy: 0.9768\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0096 - accuracy: 0.9984 - val_loss: 0.5106 - val_accuracy: 0.9758\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0087 - accuracy: 0.9986 - val_loss: 0.4954 - val_accuracy: 0.9763\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0086 - accuracy: 0.9986 - val_loss: 0.5053 - val_accuracy: 0.9771\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0087 - accuracy: 0.9986 - val_loss: 0.5136 - val_accuracy: 0.9764\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0118 - accuracy: 0.9981 - val_loss: 0.5453 - val_accuracy: 0.9771\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.5203 - val_accuracy: 0.9765\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.4701 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.5656 - val_accuracy: 0.9757\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.5410 - val_accuracy: 0.9763\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.5127 - val_accuracy: 0.9765\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0087 - accuracy: 0.9986 - val_loss: 0.4952 - val_accuracy: 0.9770\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 0.5505 - val_accuracy: 0.9770\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.5783 - val_accuracy: 0.9777\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.5018 - val_accuracy: 0.9778\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0116 - accuracy: 0.9981 - val_loss: 0.5380 - val_accuracy: 0.9762\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.5461 - val_accuracy: 0.9763\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.5773 - val_accuracy: 0.9779\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0102 - accuracy: 0.9984 - val_loss: 0.5670 - val_accuracy: 0.9758\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0115 - accuracy: 0.9981 - val_loss: 0.5418 - val_accuracy: 0.9771\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0094 - accuracy: 0.9984 - val_loss: 0.5127 - val_accuracy: 0.9779\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0097 - accuracy: 0.9984 - val_loss: 0.5383 - val_accuracy: 0.9793\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0084 - accuracy: 0.9984 - val_loss: 0.5620 - val_accuracy: 0.9762\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0112 - accuracy: 0.9984 - val_loss: 0.5980 - val_accuracy: 0.9761\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.5391 - val_accuracy: 0.9768\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0103 - accuracy: 0.9986 - val_loss: 0.5370 - val_accuracy: 0.9769\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0126 - accuracy: 0.9987 - val_loss: 0.5455 - val_accuracy: 0.9772\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.5738 - val_accuracy: 0.9777\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.5409 - val_accuracy: 0.9772\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.5540 - val_accuracy: 0.9770\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 0.0101 - accuracy: 0.9984 - val_loss: 0.5752 - val_accuracy: 0.9780\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 2s 49us/sample - loss: 0.0122 - accuracy: 0.9982 - val_loss: 0.5635 - val_accuracy: 0.9762\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0106 - accuracy: 0.9984 - val_loss: 0.6905 - val_accuracy: 0.9758\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0134 - accuracy: 0.9980 - val_loss: 0.5760 - val_accuracy: 0.9750\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 3s 53us/sample - loss: 0.0129 - accuracy: 0.9979 - val_loss: 0.5824 - val_accuracy: 0.9754\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 2s 51us/sample - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.5783 - val_accuracy: 0.9758\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.5953 - val_accuracy: 0.9764\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0103 - accuracy: 0.9984 - val_loss: 0.6075 - val_accuracy: 0.9757\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.6301 - val_accuracy: 0.9772\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0100 - accuracy: 0.9982 - val_loss: 0.6213 - val_accuracy: 0.9762\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0101 - accuracy: 0.9984 - val_loss: 0.6452 - val_accuracy: 0.9767\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0129 - accuracy: 0.9980 - val_loss: 0.5959 - val_accuracy: 0.9753\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0126 - accuracy: 0.9984 - val_loss: 0.6108 - val_accuracy: 0.9763\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.5738 - val_accuracy: 0.9750\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 2s 46us/sample - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.6201 - val_accuracy: 0.9758\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 2s 48us/sample - loss: 0.0107 - accuracy: 0.9984 - val_loss: 0.6105 - val_accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22e1957fb08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "        batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "        verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 69us/sample - loss: 0.4640 - accuracy: 0.9762\n",
      "Test accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "- for simple the model, not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter2 TensorFlow 1.x and 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\mytf\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "in_a = tf.placeholder(dtype=tf.float32, shape=(2))\n",
    "\n",
    "def model(x):\n",
    "    with tf.variable_scope(\"matmul\"):\n",
    "        W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2,2)))\n",
    "        b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2)))\n",
    "        return x * W + b\n",
    "\n",
    "out_a = model(in_a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outs = sess.run([out_a],\n",
    "            feed_dict={in_a: [1, 0]})\n",
    "    writer = tf.summary.FileWriter(\"./logs/example\", sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0\n",
    "- As discussed, TensorFlow 2.x recommends using a high-level API such as tf.keras,\n",
    "but leaves low-level APIs typical of TensorFlow 1.x for when there is a need to have\n",
    "more control on internal details.\n",
    "\n",
    "- eagar execution\n",
    "    - you still have a graph, but you can define, change, and execute nodes on-the-fly, with\n",
    "no special session interfaces or placeholders. This is what is called eager execution,\n",
    "\n",
    "- AutoGraph\n",
    "    - AutoGraph takes eager-style Python code and automatically converts it to graph-generating code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "\n",
    "- The Functional API is useful when you want to build a model with more complex\n",
    "(non-linear) topologies, including multiple inputs, multiple outputs, residual\n",
    "connections with non-sequential flows, and shared and reusable layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model subclassing\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "- Callbacks are objects passed to a model to extend or modify behaviors during\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a model and weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Estimators\n",
    "- Canned Estimators\n",
    "    - ready to use\n",
    "- custom estimators\n",
    "    - user-defined\n",
    "- The feature_column module of TensorFlow 2.0 acts as a bridge between your input data and the model\n",
    "- feature columns\n",
    "- The data for training, evaluation, as well as prediction, needs to be made available through an input function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "numeric_column = fc.numeric_column\n",
    "categorical_column_with_vocabulary_list = fc.categorical_column_with_vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featcols = [\n",
    "tf.feature_column.numeric_column(\"area\"),\n",
    "tf.feature_column.categorical_column_with_vocabulary_list(\"type\",[\n",
    "\"bungalow\",\"apartment\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    features = {\"area\":[1000,2000,4000,1000,2000,4000],\n",
    "                \"type\":[\"bungalow\",\"bungalow\",\"house\",\n",
    "                \"apartment\",\"apartment\",\"apartment\"]}\n",
    "    labels = [ 500 , 1000 , 1500 , 700 , 1300 , 1900 ]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\dell-pc\\AppData\\Local\\Temp\\tmp44jqd10w\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\dell-pc\\\\AppData\\\\Local\\\\Temp\\\\tmp44jqd10w', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\mytf\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\mytf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\mytf\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:518: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\mytf\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\ftrl.py:143: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\dell-pc\\AppData\\Local\\Temp\\tmp44jqd10w\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1548333.4, step = 1\n",
      "INFO:tensorflow:global_step/sec: 403.682\n",
      "INFO:tensorflow:loss = 54075.324, step = 101 (0.245 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\dell-pc\\AppData\\Local\\Temp\\tmp44jqd10w\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 53623.418.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x22e19bc1248>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.estimator.LinearRegressor(featcols)\n",
    "model.train(train_input_fn, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\dell-pc\\AppData\\Local\\Temp\\tmp44jqd10w\\model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([692.7829], dtype=float32)}\n",
      "{'predictions': array([830.9035], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def predict_input_fn():\n",
    "    features = {\"area\":[1500,1800],\n",
    "    \"type\":[\"house\",\"apt\"]}\n",
    "    return features\n",
    "\n",
    "predictions = model.predict(predict_input_fn)\n",
    "print(next(predictions))\n",
    "print(next(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "- Reference:\n",
    "    - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
    "\n",
    "- motivation:\n",
    "    - transfer MNIST dataset one image to 28*28 pixels is removing the spatial structure.\n",
    "    - Convolutional neural networks (in short, convnets or CNNs) leverage spatial information\n",
    "    - visual cortex: 视皮质\n",
    "- Convolutional:\n",
    "    - 深度理解卷积：https://mlnotebook.github.io/post/CNN1/\n",
    "    - https://www.zhihu.com/question/22298352\n",
    "- Three kye concepts:\n",
    "    - 1. Local receptive fields\n",
    "    - 2. shared weights\n",
    "    - 3. pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local receptive fields\n",
    "- convolusiton:\n",
    "    - connect a submatrix of adjacent input neurons into one single hidden neuron belonging to the next layer\n",
    "- In Keras, the number of pixels along one edge of the kernel, or submatrix, is the kernel size    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared weights and bias\n",
    "\n",
    "- all neuronswithin a given feature map share the same parameters (i.e., the same weights and bias term). Neurons in different feature maps use different parameters. A neuron’s receptive field is the same as described earlier, but it extends across all the previous layers’feature maps. In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Maps\n",
    "\n",
    "- every map in each layers has its own bias, b_k\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A mathematical example\n",
    "\n",
    "- kernel is a matrix\n",
    "    - convolution kernels\n",
    "    - A neuron’s weights can be represented as a small image the size of the receptive field.\n",
    "- padding: append zeros to the input\n",
    "    - In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, as shown in the diagram. This is called zero padding.\n",
    "- stride: how far along we slide our sliding windows with each step\n",
    "    - The shift from one receptive field to the next is called the stride.\n",
    "- The size of the filter, the stride, and the type of padding are hyperparameters that can be fine-tuned during the training of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "- all the pooling operations are nothing more than a summary operation on a given region.\n",
    "- However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a ResNet-34 CNN Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self,filters,strides=1,activation=\"relu\",**kwargs): #这样才是继承父类关键词参数的方法。\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "        self.main_layers = [\n",
    "            keras.layers.Conv2D(filters,3,strides=strides,\n",
    "                               padding='same',use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters,3,strides=1,\n",
    "                               padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization()\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytf",
   "language": "python",
   "name": "mytf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
