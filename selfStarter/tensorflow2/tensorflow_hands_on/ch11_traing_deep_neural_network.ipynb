{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problems\n",
    "- As a result, the Gradient Descent update leaves the lowerlayers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the vanishing gradients problem\n",
    "- the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
    "- for alleviating this problem, the connection weights of each layer must be initialized randomly \n",
    "```\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "```\n",
    "- ReLU: a problem called dying ReLU\n",
    "    - the weighted sum of itsinputs are negative for all instances in the training set.\n",
    "    - use a variant: Leaky ReLU, for z<0, give a small slope 0.01 or 0.2.\n",
    "- In general: general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- The technique consists of adding an operation in the model just before or after the activation function of each hidden layer.\n",
    "- if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with Keras\n",
    "- Just add a BatchNormalization layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model.\n",
    "- add BN before or after activation function is under debate, depends on the task\n",
    "- hyperparameter:\n",
    "    - momentum close to 1, \n",
    "    - axis: default -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "- Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "- It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle (we will discuss how to find them in Chapter 14), then reuse the lower layers of this network.\n",
    "- This technique is called transfer learning.\n",
    "- you will usually have to add a preprocessing step to resize them to the size expected by the original model.\n",
    "- Try freezing all the reused layers first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras \n",
    "- clone model to avoid train B affect A\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining\n",
    "- GANs rather than RBMs\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an Auxiliary Task\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "- Momentum optimization cares a great deal about what previous gradients were\n",
    "- the algorithm introduces a new hyperparameter β, called the momentum, which must be set between 0 (high friction) and 1 (no friction).\n",
    "- The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.\n",
    "\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization\n",
    "- Table 11-2. Optimizer comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "- reduce the learning rate during training\n",
    "- Power scheduling\n",
    "- Exponential scheduling\n",
    "- Implementing power scheduling in Keras is the easiest option: just set the decay hyperparameter when creating an optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "- Just like you did in Chapter 4 for simple linear models, you can use ℓ2 regularization to constrain a neural network’s connection weights, and/or ℓ1 regularization if you want a sparse model (with many weights equal to 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "- is one of the most popular regularization techniques for deep neural networks.\n",
    "- Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization\n",
    "- computing ∥w∥2 after each training step and rescaling w if needed (w ← w r/‖ w ‖2).\n",
    "- Reducing r increases the amount of regularization and helps reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUmmary\n",
    "- Table 11-3. Default DNN configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytf",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
